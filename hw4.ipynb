{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPBgyv3iQhgHxOlSFiskBl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junaid537/pytorch/blob/main/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_qVoOBuDN7",
        "outputId": "955deefd-c4b3-499a-c3eb-98b36531bb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\""
      ],
      "metadata": {
        "id": "qh_BftaBsCOF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_bV7DC5jt4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\""
      ],
      "metadata": {
        "id": "KP8-EFVG8Jl8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Paths\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "# output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# # Optimized Hyperparameters\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0.2\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.01\n",
        "# MOMENTUM = 0.9\n",
        "# WEIGHT_DECAY = 5e-5\n",
        "# EPOCHS = 30\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Load dataset\n",
        "# def load_data(filename, is_test=False):\n",
        "#     sentences, labels = [], []\n",
        "#     with open(filename, 'r', encoding='utf-8') as f:\n",
        "#         sentence, label = [], []\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if not line:\n",
        "#                 if sentence:\n",
        "#                     sentences.append(sentence)\n",
        "#                     labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 continue\n",
        "#             parts = line.split()\n",
        "#             if len(parts) < 2:\n",
        "#                 continue\n",
        "#             sentence.append(parts[1])\n",
        "#             if not is_test:\n",
        "#                 label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "#             else:\n",
        "#                 label.append(\"O\")\n",
        "#     if sentence:\n",
        "#         sentences.append(sentence)\n",
        "#         labels.append(label)\n",
        "#     return sentences, labels\n",
        "\n",
        "# # Load train, dev, and test data\n",
        "# train_sentences, train_labels = load_data(train_file)\n",
        "# dev_sentences, dev_labels = load_data(dev_file)\n",
        "# test_sentences, _ = load_data(test_file, is_test=True)  # No labels for test set\n",
        "\n",
        "# # Build vocabulary\n",
        "# word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "# tag_to_ix = {}\n",
        "# for sentence in train_sentences:\n",
        "#     for word in sentence:\n",
        "#         if word not in word_to_ix:\n",
        "#             word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# for labels in train_labels:\n",
        "#     for tag in labels:\n",
        "#         if tag not in tag_to_ix:\n",
        "#             tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# # Dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "#         self.sentences = sentences\n",
        "#         self.labels = labels\n",
        "#         self.word_to_ix = word_to_ix\n",
        "#         self.tag_to_ix = tag_to_ix\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sentence = self.sentences[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "#         label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to 0 for unknown tags\n",
        "#         return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# # Custom collate function\n",
        "# def collate_fn(batch):\n",
        "#     words, labels = zip(*batch)\n",
        "#     words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "#     labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "#     return words_padded, labels_padded\n",
        "\n",
        "# # Create datasets and loaders\n",
        "# train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "# dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "# test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)  # Dummy labels for test\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)  # FIXED: Now test_loader is defined!\n",
        "\n",
        "# # Define BLSTM Model\n",
        "# class BLSTM_NER(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size):\n",
        "#         super(BLSTM_NER, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "#                             bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.fc(x)\n",
        "#         x = self.elu(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, loss, optimizer, and scheduler\n",
        "# model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for words, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(words)\n",
        "\n",
        "#         # Reshape for loss computation\n",
        "#         outputs = outputs.view(-1, outputs.shape[-1])\n",
        "#         labels = labels.view(-1).long()\n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     scheduler.step()\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# # Save model\n",
        "# torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "# print(\"Model saved!\")\n",
        "\n",
        "# # Evaluation function\n",
        "# def evaluate(model, data_loader, sentences, output_file):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     batch_offset = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for words, _ in data_loader:\n",
        "#             outputs = model(words)\n",
        "#             outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "#             for batch_idx, preds in enumerate(outputs):\n",
        "#                 sentence = sentences[batch_offset]\n",
        "#                 for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "#                     predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "#                 predictions.append(\"\\n\")\n",
        "#                 batch_offset += 1\n",
        "\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.writelines(predictions)\n",
        "\n",
        "#     print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# # Evaluate on dev set\n",
        "# evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# # Evaluate on test set\n",
        "# evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "# print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "id": "rT7_IYhlVJW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# Optimized Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.12  # Lower dropout to improve recall\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01  # More stable learning\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-5  # Less regularization to prevent underfitting\n",
        "EPOCHS = 35  # Train longer for better F1\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)  # No labels for test set\n",
        "\n",
        "# Build vocabulary\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to 0 for unknown tags\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)  # Dummy labels for test\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# âœ… Learning Rate Scheduler (Reduce LR every 10 epochs instead of 5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "\n",
        "        # Reshape for loss computation\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "    print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# Evaluate on dev set\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xEQooxdVJU7",
        "outputId": "43324669-cb15-473b-dcde-4b54aa85ee92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 364.3246, LR: 0.01\n",
            "Epoch 2, Loss: 316.1281, LR: 0.01\n",
            "Epoch 3, Loss: 290.7997, LR: 0.01\n",
            "Epoch 4, Loss: 261.6946, LR: 0.01\n",
            "Epoch 5, Loss: 232.1846, LR: 0.01\n",
            "Epoch 6, Loss: 209.5106, LR: 0.01\n",
            "Epoch 7, Loss: 192.0653, LR: 0.01\n",
            "Epoch 8, Loss: 177.5068, LR: 0.01\n",
            "Epoch 9, Loss: 164.7305, LR: 0.01\n",
            "Epoch 10, Loss: 152.8905, LR: 0.01\n",
            "Epoch 11, Loss: 142.2381, LR: 0.01\n",
            "Epoch 12, Loss: 132.0019, LR: 0.01\n",
            "Epoch 13, Loss: 122.4524, LR: 0.01\n",
            "Epoch 14, Loss: 113.9070, LR: 0.01\n",
            "Epoch 15, Loss: 105.0443, LR: 0.008\n",
            "Epoch 16, Loss: 96.6448, LR: 0.008\n",
            "Epoch 17, Loss: 89.9495, LR: 0.008\n",
            "Epoch 18, Loss: 84.2575, LR: 0.008\n",
            "Epoch 19, Loss: 78.6785, LR: 0.008\n",
            "Epoch 20, Loss: 73.1632, LR: 0.008\n",
            "Epoch 21, Loss: 67.9451, LR: 0.008\n",
            "Epoch 22, Loss: 63.3880, LR: 0.008\n",
            "Epoch 23, Loss: 58.5493, LR: 0.008\n",
            "Epoch 24, Loss: 54.2120, LR: 0.008\n",
            "Epoch 25, Loss: 50.1985, LR: 0.008\n",
            "Epoch 26, Loss: 46.4349, LR: 0.008\n",
            "Epoch 27, Loss: 43.1419, LR: 0.008\n",
            "Epoch 28, Loss: 39.6696, LR: 0.008\n",
            "Epoch 29, Loss: 36.8567, LR: 0.008\n",
            "Epoch 30, Loss: 34.1144, LR: 0.0064\n",
            "Epoch 31, Loss: 30.9396, LR: 0.0064\n",
            "Epoch 32, Loss: 29.2086, LR: 0.0064\n",
            "Epoch 33, Loss: 27.5026, LR: 0.0064\n",
            "Epoch 34, Loss: 25.9152, LR: 0.0064\n",
            "Epoch 35, Loss: 24.2477, LR: 0.0064\n",
            "Model saved!\n",
            "Saved predictions to /content/drive/MyDrive/dataHW4csci544/dev.out\n",
            "Saved predictions to /content/drive/MyDrive/dataHW4csci544/test.out\n",
            "Training completed. Model and outputs saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code with 70 F1\n",
        "#using SGD\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Paths\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "# output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# # Hyperparameters\n",
        "# # EMBEDDING_DIM = 100\n",
        "# # HIDDEN_DIM = 256\n",
        "# # LSTM_LAYERS = 1\n",
        "# # DROPOUT = 0.33\n",
        "# # LINEAR_OUT_DIM = 128\n",
        "# # BATCH_SIZE = 32\n",
        "# # LEARNING_RATE = 0.1  # Adjusted for SGD\n",
        "# # MOMENTUM = 0.9  # Momentum for SGD\n",
        "# # WEIGHT_DECAY = 1e-4  # L2 Regularization\n",
        "# # EPOCHS = 10  # Increased for better training\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0.2\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.01  # Adjusted for SGD\n",
        "# MOMENTUM = 0.9  # Momentum for SGD\n",
        "# WEIGHT_DECAY = 5e-5  # L2 Regularization\n",
        "# EPOCHS = 20  # Increased for better training\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Load dataset (Handles both train/dev and test files)\n",
        "# def load_data(filename, is_test=False):\n",
        "#     sentences, labels = [], []\n",
        "#     with open(filename, 'r', encoding='utf-8') as f:\n",
        "#         sentence, label = [], []\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if not line:\n",
        "#                 if sentence:\n",
        "#                     sentences.append(sentence)\n",
        "#                     labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 continue\n",
        "#             parts = line.split()\n",
        "#             if len(parts) < 2:\n",
        "#                 continue  # Skip malformed lines\n",
        "#             sentence.append(parts[1])  # Word\n",
        "#             if not is_test:\n",
        "#                 label.append(parts[2] if len(parts) > 2 else \"O\")  # NER Tag\n",
        "#             else:\n",
        "#                 label.append(\"O\")  # Assign \"O\" for test set\n",
        "#     if sentence:\n",
        "#         sentences.append(sentence)\n",
        "#         labels.append(label)\n",
        "#     return sentences, labels\n",
        "\n",
        "# # Load train and dev data\n",
        "# train_sentences, train_labels = load_data(train_file)\n",
        "# dev_sentences, dev_labels = load_data(dev_file)\n",
        "\n",
        "# # Build vocabulary\n",
        "# word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "# tag_to_ix = {}\n",
        "# for sentence in train_sentences:\n",
        "#     for word in sentence:\n",
        "#         if word not in word_to_ix:\n",
        "#             word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# for labels in train_labels:\n",
        "#     for tag in labels:\n",
        "#         if tag not in tag_to_ix:\n",
        "#             tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# # Dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "#         self.sentences = sentences\n",
        "#         self.labels = labels\n",
        "#         self.word_to_ix = word_to_ix\n",
        "#         self.tag_to_ix = tag_to_ix\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sentence = self.sentences[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "#         label_indices = [self.tag_to_ix[l] for l in label]\n",
        "#         return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# # Custom collate function for dynamic padding\n",
        "# def collate_fn(batch):\n",
        "#     words, labels = zip(*batch)\n",
        "#     words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "#     labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "#     return words_padded, labels_padded\n",
        "\n",
        "# # Create datasets and loaders\n",
        "# train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "# dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Define BLSTM Model\n",
        "# class BLSTM_NER(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size):\n",
        "#         super(BLSTM_NER, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "#                             bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.fc(x)\n",
        "#         x = self.elu(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, loss, and optimizer (Using SGD Now)\n",
        "# model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# # **Using SGD Instead of Adam**\n",
        "# #optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "# # Learning Rate Scheduler (Reduce LR every 5 epochs)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "# # Training loop (Fixed)\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for words, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(words)  # [batch_size, max_seq_len, num_classes]\n",
        "\n",
        "#         # Reshape for loss computation\n",
        "#         outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * max_seq_len, num_classes]\n",
        "#         labels = labels.view(-1).long()  # [batch_size * max_seq_len]\n",
        "\n",
        "#         # Ensure loss ignores padding labels (-1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# # Save model\n",
        "# torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "# print(\"Model saved!\")\n",
        "\n",
        "# # Evaluation function (Fixed Indexing)\n",
        "# def evaluate(model, data_loader, sentences, output_file):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     batch_offset = 0  # Ensure correct sentence order\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for words, _ in data_loader:\n",
        "#             outputs = model(words)\n",
        "#             outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "#             for batch_idx, preds in enumerate(outputs):\n",
        "#                 sentence = sentences[batch_offset]\n",
        "#                 for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "#                     predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "#                 predictions.append(\"\\n\")  # Ensure sentence separation\n",
        "#                 batch_offset += 1  # Move to next sentence\n",
        "\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.writelines(predictions)\n",
        "\n",
        "#     print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# # Evaluate on dev set\n",
        "# evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# # Evaluate on test set\n",
        "# test_sentences, test_labels = load_data(test_file, is_test=True)\n",
        "# test_dataset = NERDataset(test_sentences, test_labels, word_to_ix, tag_to_ix)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "# evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "# print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "id": "lhApZjhCIRas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USing Adams\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Paths\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "# output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# # Hyperparameters\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0.33\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.001  # Reduced for better stability\n",
        "# EPOCHS = 5  # Increased for better training\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Load dataset (Handles both train/dev and test files)\n",
        "# def load_data(filename, is_test=False):\n",
        "#     sentences, labels = [], []\n",
        "#     with open(filename, 'r', encoding='utf-8') as f:\n",
        "#         sentence, label = [], []\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if not line:\n",
        "#                 if sentence:\n",
        "#                     sentences.append(sentence)\n",
        "#                     labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 continue\n",
        "#             parts = line.split()\n",
        "#             if len(parts) < 2:\n",
        "#                 continue  # Skip malformed lines\n",
        "#             sentence.append(parts[1])  # Word\n",
        "#             if not is_test:\n",
        "#                 label.append(parts[2] if len(parts) > 2 else \"O\")  # NER Tag\n",
        "#             else:\n",
        "#                 label.append(\"O\")  # Assign \"O\" for test set\n",
        "#     if sentence:\n",
        "#         sentences.append(sentence)\n",
        "#         labels.append(label)\n",
        "#     return sentences, labels\n",
        "\n",
        "# # Load train and dev data\n",
        "# train_sentences, train_labels = load_data(train_file)\n",
        "# dev_sentences, dev_labels = load_data(dev_file)\n",
        "\n",
        "# # Build vocabulary\n",
        "# word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "# tag_to_ix = {}\n",
        "# for sentence in train_sentences:\n",
        "#     for word in sentence:\n",
        "#         if word not in word_to_ix:\n",
        "#             word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# for labels in train_labels:\n",
        "#     for tag in labels:\n",
        "#         if tag not in tag_to_ix:\n",
        "#             tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# # Dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "#         self.sentences = sentences\n",
        "#         self.labels = labels\n",
        "#         self.word_to_ix = word_to_ix\n",
        "#         self.tag_to_ix = tag_to_ix\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sentence = self.sentences[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "#         label_indices = [self.tag_to_ix[l] for l in label]\n",
        "#         return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# # Custom collate function for dynamic padding\n",
        "# def collate_fn(batch):\n",
        "#     words, labels = zip(*batch)\n",
        "#     words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "#     labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "#     return words_padded, labels_padded\n",
        "\n",
        "# # Create datasets and loaders\n",
        "# train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "# dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Define BLSTM Model\n",
        "# class BLSTM_NER(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size):\n",
        "#         super(BLSTM_NER, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "#                             bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.fc(x)\n",
        "#         x = self.elu(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, loss, and optimizer\n",
        "# model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Switched to Adam for better convergence\n",
        "\n",
        "# # Training loop (Fixed)\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for words, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(words)  # [batch_size, max_seq_len, num_classes]\n",
        "\n",
        "#         # Reshape for loss computation\n",
        "#         outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * max_seq_len, num_classes]\n",
        "#         labels = labels.view(-1).long()  # [batch_size * max_seq_len]\n",
        "\n",
        "#         # Ensure loss ignores padding labels (-1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# # Save model\n",
        "# torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "# print(\"Model saved!\")\n",
        "\n",
        "# # Evaluation function (Fixed Indexing)\n",
        "# def evaluate(model, data_loader, sentences, output_file):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     batch_offset = 0  # Ensure correct sentence order\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for words, _ in data_loader:\n",
        "#             outputs = model(words)\n",
        "#             outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "#             for batch_idx, preds in enumerate(outputs):\n",
        "#                 sentence = sentences[batch_offset]\n",
        "#                 for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "#                     predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")  # Start from 1\n",
        "#                 predictions.append(\"\\n\")  # Ensure sentence separation\n",
        "#                 batch_offset += 1  # Move to next sentence\n",
        "\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.writelines(predictions)\n",
        "\n",
        "#     print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "\n",
        "# # Evaluate on dev set\n",
        "# evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# # Load and process test data\n",
        "# test_sentences, test_labels = load_data(test_file, is_test=True)\n",
        "# test_dataset = NERDataset(test_sentences, test_labels, word_to_ix, tag_to_ix)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Evaluate on test set\n",
        "# evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "# print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "id": "zJ3o8OTaEF1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/dataHW4csci544/eval.py -p /content/drive/MyDrive/dataHW4csci544/dev.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "5d8G2hjeJmLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6599a066-857b-47ba-eee6-7cb142bb9e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 4804 phrases; correct: 3473.\n",
            "accuracy:  92.94%; precision:  72.29%; recall:  58.45%; FB1:  64.64\n",
            "              LOC: precision:  83.68%; recall:  76.21%; FB1:  79.77  1673\n",
            "             MISC: precision:  69.60%; recall:  60.85%; FB1:  64.93  806\n",
            "              ORG: precision:  68.95%; recall:  54.81%; FB1:  61.07  1066\n",
            "              PER: precision:  61.72%; recall:  42.18%; FB1:  50.11  1259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python eval.py -p /content/drive/MyDrive/dataHW4csci544/dev.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "Rq6Bn7F2ImZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83rKGeO1ImW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0_4l-m0ImUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjZ9ZVOaImR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#working\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Define the dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, file_path, word_to_idx, label_to_idx):\n",
        "#         self.word_to_idx = word_to_idx\n",
        "#         self.label_to_idx = label_to_idx\n",
        "#         self.sentences, self.labels = self.load_data(file_path)\n",
        "\n",
        "#     def load_data(self, file_path):\n",
        "#         sentences = []\n",
        "#         labels = []\n",
        "\n",
        "#         with open(file_path, 'r') as file:\n",
        "#             sentence = []\n",
        "#             label = []\n",
        "\n",
        "#             for line in file:\n",
        "#                 line = line.strip()\n",
        "#                 if line == \"\":\n",
        "#                     if sentence:\n",
        "#                         sentences.append(sentence)\n",
        "#                         labels.append(label)\n",
        "#                     sentence = []\n",
        "#                     label = []\n",
        "#                 else:\n",
        "#                     parts = line.split()\n",
        "#                     if len(parts) == 3:\n",
        "#                         idx, word, gold_label = parts\n",
        "#                     elif len(parts) == 2:\n",
        "#                         idx, word = parts\n",
        "#                         gold_label = \"O\"  # Default label if missing\n",
        "#                     else:\n",
        "#                         continue\n",
        "\n",
        "#                     sentence.append(self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]))\n",
        "#                     label.append(self.label_to_idx.get(gold_label, self.label_to_idx[\"O\"]))\n",
        "\n",
        "#         return sentences, labels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "# # Collate function to pad sequences\n",
        "# def collate_fn(batch):\n",
        "#     sentences, labels = zip(*batch)\n",
        "#     padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "#     padded_labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "#     return padded_sentences, padded_labels\n",
        "\n",
        "# # Define the BLSTM model\n",
        "# class BLSTMModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "#         super(BLSTMModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True, dropout=0.33)\n",
        "#         self.fc = nn.Linear(hidden_size * 2, 128)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(128, output_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         lstm_out, _ = self.lstm(x)\n",
        "#         linear_out = self.fc(lstm_out)\n",
        "#         elu_out = self.elu(linear_out)\n",
        "#         logits = self.classifier(elu_out)\n",
        "#         return logits\n",
        "\n",
        "# # Training function\n",
        "# def train_model(train_loader, model, criterion, optimizer, num_epochs=5):\n",
        "#     model.train()\n",
        "#     for epoch in range(num_epochs):\n",
        "#         total_loss = 0\n",
        "#         for sentences, labels in train_loader:\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(sentences)\n",
        "#             loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "#             total_loss += loss.item()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# # Save predictions in conll03eval format\n",
        "# def save_predictions(predictions, data_file, output_file):\n",
        "#     with open(data_file, 'r') as f, open(output_file, 'w') as out_f:\n",
        "#         i = 0\n",
        "#         predictions = [pred.item() for batch in predictions for pred in batch.flatten()]\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if line:\n",
        "#                 parts = line.split()\n",
        "#                 if len(parts) >= 2:\n",
        "#                     idx, word = parts[:2]\n",
        "#                     pred = predictions[i]\n",
        "#                     out_f.write(f\"{idx} {word} {pred}\\n\")\n",
        "#                     i += 1\n",
        "#             else:\n",
        "#                 out_f.write(\"\\n\")\n",
        "\n",
        "# # Make predictions\n",
        "# def make_predictions(data_loader, model):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     with torch.no_grad():\n",
        "#         for sentences, _ in data_loader:\n",
        "#             outputs = model(sentences)\n",
        "#             _, predicted_labels = torch.max(outputs, -1)\n",
        "#             predictions.extend(predicted_labels)\n",
        "#     return predictions\n",
        "\n",
        "# # Prepare word and label mappings\n",
        "# word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special tokens\n",
        "# label_to_idx = {}\n",
        "\n",
        "# # Load training data to populate mappings\n",
        "# train_data_path = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# with open(train_data_path, 'r') as f:\n",
        "#     for line in f:\n",
        "#         line = line.strip()\n",
        "#         if line:\n",
        "#             _, word, label = line.split()\n",
        "#             if word not in word_to_idx:\n",
        "#                 word_to_idx[word] = len(word_to_idx)\n",
        "#             if label not in label_to_idx:\n",
        "#                 label_to_idx[label] = len(label_to_idx)\n",
        "\n",
        "# idx_to_label = {idx: label for label, idx in label_to_idx.items()}  # Reverse mapping\n",
        "\n",
        "# # Load datasets\n",
        "# train_data = NERDataset(train_data_path, word_to_idx, label_to_idx)\n",
        "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# # Model setup\n",
        "# input_size = len(word_to_idx)\n",
        "# embedding_dim = 100\n",
        "# hidden_size = 256\n",
        "# output_size = len(label_to_idx)\n",
        "# model = BLSTMModel(input_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "# # Loss and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# # Train the model\n",
        "# train_model(train_loader, model, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "# # Save the trained model\n",
        "# model_save_path = \"/content/drive/MyDrive/dataHW4csci544/blstm1.pt\"\n",
        "# torch.save(model.state_dict(), model_save_path)\n",
        "# print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# # Evaluate on Dev and Test Sets\n",
        "# dev_data_path = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_data_path = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "\n",
        "# dev_data = NERDataset(dev_data_path, word_to_idx, label_to_idx)\n",
        "# test_data = NERDataset(test_data_path, word_to_idx, label_to_idx)\n",
        "\n",
        "# dev_loader = DataLoader(dev_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "# test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Get predictions\n",
        "# dev_predictions = make_predictions(dev_loader, model)\n",
        "# test_predictions = make_predictions(test_loader, model)\n",
        "\n",
        "# # Save predictions\n",
        "# save_predictions(dev_predictions, dev_data_path, \"/content/drive/MyDrive/dataHW4csci544/dev.out\")\n",
        "# save_predictions(test_predictions, test_data_path, \"/content/drive/MyDrive/dataHW4csci544/test.out\")\n",
        "# print(\"Predictions saved.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PiJ-nZttdekS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluate using conll03eval\n",
        "# eval_command = \"python /content/drive/MyDrive/dataHW4csci544/eval.py -p /content/drive/MyDrive/dataHW4csci544/dev.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# os.system(eval_command)\n"
      ],
      "metadata": {
        "id": "Hqo5tlEsqRVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 '/content/drive/MyDrive/dataHW4csci544/eval.py' -p '/content/drive/MyDrive/dataHW4csci544/dev.out' -g '/content/drive/MyDrive/dataHW4csci544/data/dev'\n",
        "\n"
      ],
      "metadata": {
        "id": "IiEkT68ldee7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k2zGKqYpt0NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/dataHW4csci544/eval.py -p /content/drive/MyDrive/dataHW4csci544/dev.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "qzQ4eAxPdecD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/dataHW4csci544/'\n"
      ],
      "metadata": {
        "id": "dt7erMQNdeZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4kyi7irdeWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CiVYI6MtdeTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8R5u15vLdeRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzqTRsNDdeOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ApNIgB5deL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#working\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import numpy as np\n",
        "\n",
        "# # Hyperparameters\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# DROPOUT = 0.33\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# NUM_LSTM_LAYERS = 1\n",
        "# BATCH_SIZE = 32  # To be tuned\n",
        "# LEARNING_RATE = 0.01  # To be tuned\n",
        "# NUM_EPOCHS = 10  # To be tuned\n",
        "\n",
        "# # Load Vocabulary and Labels\n",
        "# def build_vocab_and_labels(file_path):\n",
        "#     word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "#     label_vocab = {}\n",
        "\n",
        "#     with open(file_path, 'r') as f:\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if line:\n",
        "#                 _, word, label = line.split()\n",
        "#                 if word not in word_vocab:\n",
        "#                     word_vocab[word] = len(word_vocab)\n",
        "#                 if label not in label_vocab:\n",
        "#                     label_vocab[label] = len(label_vocab)\n",
        "\n",
        "#     return word_vocab, label_vocab\n",
        "\n",
        "# # Dataset Class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, file_path, word_vocab, label_vocab):\n",
        "#         self.sentences, self.labels = self.load_data(file_path, word_vocab, label_vocab)\n",
        "\n",
        "#     def load_data(self, file_path, word_vocab, label_vocab):\n",
        "#         sentences, labels = [], []\n",
        "#         sentence, label = [], []\n",
        "\n",
        "#         with open(file_path, 'r') as f:\n",
        "#             for line in f:\n",
        "#                 line = line.strip()\n",
        "#                 if line == \"\":\n",
        "#                     if sentence:\n",
        "#                         sentences.append(sentence)\n",
        "#                         labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 else:\n",
        "#                     _, word, ner_tag = line.split()\n",
        "#                     sentence.append(word_vocab.get(word, word_vocab[\"<UNK>\"]))\n",
        "#                     label.append(label_vocab[ner_tag])\n",
        "\n",
        "#         return sentences, labels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "# # Collate function for batching\n",
        "# def collate_fn(batch):\n",
        "#     sentences, labels = zip(*batch)\n",
        "#     padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "#     padded_labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "#     return padded_sentences, padded_labels\n",
        "\n",
        "# # BLSTM Model\n",
        "# class BLSTMModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, num_labels):\n",
        "#         super(BLSTMModel, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM, padding_idx=0)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, NUM_LSTM_LAYERS,\n",
        "#                             batch_first=True, bidirectional=True, dropout=DROPOUT)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, num_labels)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         lstm_out, _ = self.lstm(x)\n",
        "#         linear_out = self.fc(lstm_out)\n",
        "#         activated = self.elu(linear_out)\n",
        "#         output = self.classifier(activated)\n",
        "#         return output\n",
        "\n",
        "# # Training Function\n",
        "# def train_model(model, train_loader, criterion, optimizer):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for sentences, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(sentences)\n",
        "#         loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#     return total_loss / len(train_loader)\n",
        "\n",
        "# # Evaluation Function\n",
        "# def evaluate_model(model, data_loader):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     with torch.no_grad():\n",
        "#         for sentences, _ in data_loader:\n",
        "#             outputs = model(sentences)\n",
        "#             _, predicted_labels = torch.max(outputs, -1)\n",
        "#             predictions.append(predicted_labels)\n",
        "#     return predictions\n",
        "\n",
        "# # Load Data\n",
        "\n",
        "# word_vocab, label_vocab = build_vocab_and_labels(train_file)\n",
        "\n",
        "# train_data = NERDataset(train_file, word_vocab, label_vocab)\n",
        "# dev_data = NERDataset(dev_file, word_vocab, label_vocab)\n",
        "\n",
        "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Initialize Model\n",
        "# num_labels = len(label_vocab)\n",
        "# model = BLSTMModel(len(word_vocab), num_labels)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# # Train the Model\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     loss = train_model(model, train_loader, criterion, optimizer)\n",
        "#     print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n",
        "\n",
        "# # Save Model\n",
        "# torch.save(model.state_dict(), \"blstm_ner.pt\")\n",
        "\n",
        "# # Make Predictions\n",
        "# dev_predictions = evaluate_model(model, dev_loader)\n",
        "\n",
        "# # Save Predictions in conll03eval format\n",
        "# import torch\n",
        "\n",
        "# def save_predictions(predictions, data_file, output_file):\n",
        "#     with open(data_file, 'r') as f, open(output_file, 'w') as out_f:\n",
        "#         i = 0\n",
        "#         # Flatten predictions to a single list\n",
        "#         predictions = torch.cat(predictions).tolist()  # Convert tensor batches into a single list\n",
        "\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if line:  # Not an empty line\n",
        "#                 idx, word, gold = line.split()\n",
        "#                 pred = predictions[i]  # Extract prediction correctly\n",
        "#                 out_f.write(f\"{idx} {word} {pred}\\n\")\n",
        "#                 i += 1\n",
        "#             else:\n",
        "#                 out_f.write(\"\\n\")  # Separate sentences properly\n",
        "\n",
        "\n",
        "# # Run Evaluation\n",
        "# import os\n",
        "# os.system(\"python eval.py -p dev_predictions.out -g dev.txt\")\n"
      ],
      "metadata": {
        "id": "QPNch2DbVegY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvR8CTTkVeak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2aZzB9mVeXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9HgqnWxVeUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1_-_vbIVeRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M36DasTLVeOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCdrPfCUVeLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySOMoEgqVeIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHfNx6VXVeF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\""
      ],
      "metadata": {
        "id": "KmKPBh6kVeC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aZDL6_Y_vJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StULyBDB54Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#python eval.py -p /content/drive/MyDrive/dataHW4csci544/dev1.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "x1n9ojIj_k5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "kB5tFmNVqjf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_path = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"  # GloVe embeddings file\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.1\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.85\n",
        "WEIGHT_DECAY = 3e-5\n",
        "EPOCHS = 35\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract GloVe embeddings from .gz file\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with gzip.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary with GloVe embeddings\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "\n",
        "# Step 1: Create an embedding matrix\n",
        "embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix) + 50000, EMBEDDING_DIM))  # Extra space for new words\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# Assign GloVe vectors to words (handling case sensitivity)\n",
        "for word, idx in word_to_ix.items():\n",
        "    lowercase_word = word.lower()\n",
        "    if lowercase_word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[lowercase_word]\n",
        "    elif word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Map entity labels to indexes\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model with GloVe Embeddings\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_matrix):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix), embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.85, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm2.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# **Save predictions as dev2 and test2**\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "# Save outputs as dev2 and test2\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev2.out\"))\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test2.out\"))\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "ju13f8NS_k0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2e057d-b85d-4b03-a4ac-149eb09232fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 316.5207, LR: 0.01\n",
            "Epoch 2, Loss: 232.0206, LR: 0.01\n",
            "Epoch 3, Loss: 192.2139, LR: 0.01\n",
            "Epoch 4, Loss: 163.6787, LR: 0.01\n",
            "Epoch 5, Loss: 145.3240, LR: 0.01\n",
            "Epoch 6, Loss: 131.2901, LR: 0.01\n",
            "Epoch 7, Loss: 119.8484, LR: 0.01\n",
            "Epoch 8, Loss: 111.0587, LR: 0.01\n",
            "Epoch 9, Loss: 103.2982, LR: 0.01\n",
            "Epoch 10, Loss: 96.9492, LR: 0.008\n",
            "Epoch 11, Loss: 91.4502, LR: 0.008\n",
            "Epoch 12, Loss: 87.2833, LR: 0.008\n",
            "Epoch 13, Loss: 83.5890, LR: 0.008\n",
            "Epoch 14, Loss: 80.2770, LR: 0.008\n",
            "Epoch 15, Loss: 77.2407, LR: 0.008\n",
            "Epoch 16, Loss: 75.0093, LR: 0.008\n",
            "Epoch 17, Loss: 71.8924, LR: 0.008\n",
            "Epoch 18, Loss: 69.2763, LR: 0.008\n",
            "Epoch 19, Loss: 67.4815, LR: 0.008\n",
            "Epoch 20, Loss: 64.9579, LR: 0.0064\n",
            "Epoch 21, Loss: 63.2328, LR: 0.0064\n",
            "Epoch 22, Loss: 61.8117, LR: 0.0064\n",
            "Epoch 23, Loss: 60.1013, LR: 0.0064\n",
            "Epoch 24, Loss: 58.8562, LR: 0.0064\n",
            "Epoch 25, Loss: 57.3848, LR: 0.0064\n",
            "Epoch 26, Loss: 55.9593, LR: 0.0064\n",
            "Epoch 27, Loss: 55.1110, LR: 0.0064\n",
            "Epoch 28, Loss: 53.8550, LR: 0.0064\n",
            "Epoch 29, Loss: 52.7027, LR: 0.0064\n",
            "Epoch 30, Loss: 51.6905, LR: 0.00512\n",
            "Epoch 31, Loss: 50.7616, LR: 0.00512\n",
            "Epoch 32, Loss: 49.9726, LR: 0.00512\n",
            "Epoch 33, Loss: 49.0009, LR: 0.00512\n",
            "Epoch 34, Loss: 48.2148, LR: 0.00512\n",
            "Epoch 35, Loss: 47.7671, LR: 0.00512\n",
            "Model saved!\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OoPad5IqekA",
        "outputId": "13fb42fd-a7c2-453c-d548-6fb264b1013a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev2.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxu3Tujqqhj6",
        "outputId": "94d23149-f648-4771-e5cf-efdb6015ab3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5489 phrases; correct: 4428.\n",
            "accuracy:  95.51%; precision:  80.67%; recall:  74.52%; FB1:  77.47\n",
            "              LOC: precision:  88.42%; recall:  82.31%; FB1:  85.26  1710\n",
            "             MISC: precision:  78.25%; recall:  69.85%; FB1:  73.81  823\n",
            "              ORG: precision:  66.14%; recall:  64.95%; FB1:  65.54  1317\n",
            "              PER: precision:  85.48%; recall:  76.06%; FB1:  80.49  1639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/hw4csci544/eval.py\"\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(\"âœ… File exists!\")\n",
        "else:\n",
        "    print(\"âŒ File does NOT exist!\")"
      ],
      "metadata": {
        "id": "tuWx59ii_kue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549fb900-364b-4c7a-dc22-9f4b31c6c0eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdRTu8mG_krk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "nAn_h7tD3E7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8lMhX0KjWZYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "2631d604-341f-41bd-8fcc-2d8eab1ed219"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-316779821e67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_and_unmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrlUQlxMrdrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "# Paths (Google Drive)\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_file = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "model_path = os.path.join(output_dir, \"blstm_cnn_model.pt\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.2\n",
        "LINEAR_OUT_DIM = 128\n",
        "CHAR_EMBEDDING_DIM = 30\n",
        "NUM_CNN_FILTERS = 30\n",
        "CNN_KERNEL_SIZES = [3, 4, 5]\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-5\n",
        "EPOCHS = 35\n",
        "SCHEDULER_STEP = 15\n",
        "SCHEDULER_GAMMA = 0.8\n",
        "\n",
        "# Device setup (Use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset function\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabularies\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "char_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        for char in word:\n",
        "            if char not in char_to_ix:\n",
        "                char_to_ix[char] = len(char_to_ix)\n",
        "\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file, word_to_ix, embedding_dim=100):\n",
        "    embeddings = np.random.uniform(-0.1, 0.1, (len(word_to_ix), embedding_dim))\n",
        "    embeddings[0] = np.zeros(embedding_dim)  # PAD token\n",
        "    with gzip.open(glove_file, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype=np.float32)\n",
        "            if word in word_to_ix:\n",
        "                embeddings[word_to_ix[word]] = vector\n",
        "            elif word.lower() in word_to_ix:\n",
        "                embeddings[word_to_ix[word.lower()]] = vector\n",
        "    return torch.tensor(embeddings, dtype=torch.float)\n",
        "\n",
        "pretrained_embeddings = load_glove_embeddings(glove_file, word_to_ix)\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix, char_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.char_to_ix = char_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        char_indices = [[self.char_to_ix.get(c, 1) for c in word] for word in sentence]\n",
        "        label_indices = [self.tag_to_ix[l] for l in label]\n",
        "        return torch.tensor(word_indices), torch.tensor(char_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Save predictions function\n",
        "def save_predictions(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for words, chars, _ in data_loader:\n",
        "            outputs = model(words.to(device), chars.to(device))\n",
        "            outputs = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "            for i, sent in enumerate(sentences):\n",
        "                for j, word in enumerate(sent):\n",
        "                    predictions.append(f\"{j+1} {word} {ix_to_tag[outputs[i, j]]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "# Training loop\n",
        "model = BLSTM_CNN_NER(len(word_to_ix), len(tag_to_ix), len(char_to_ix)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP, gamma=SCHEDULER_GAMMA)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for words, chars, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words.to(device), chars.to(device))\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long().to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved at: {model_path}\")\n",
        "\n",
        "# Save dev and test predictions\n",
        "save_predictions(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev3.out\"))\n",
        "save_predictions(model, test_loader, test_sentences, os.path.join(output_dir, \"pred\"))\n",
        "print(\"Training completed. Predictions saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "M5KLlfu2rdnD",
        "outputId": "48af31dd-1e86-43ea-bc61-ba78ef4e088a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/hw4csci544/data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6937e2dd5fc5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Load train, dev, and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mdev_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6937e2dd5fc5>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename, is_test)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/hw4csci544/data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA9QynXprdlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1xLY2dXrdiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}