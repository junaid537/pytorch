{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junaid537/pytorch/blob/main/%E2%80%9CHW2_Junaid_Khalidi_Py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --force-reinstall nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "nXvTgKu4i9aV",
        "outputId": "41a25ad4-b36f-4227-e430-b52a63586106"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "nltk",
                  "regex",
                  "tqdm"
                ]
              },
              "id": "64bf3f3dfbf2455c8d55ee5df4eec1d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TSGYOOKQCkvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "dabb7103-79e9-47ed-c6bc-e61bfa71cd17"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk' has no attribute 'internals'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f42d784fd132>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Ensure NLTK is correctly installed and restart the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    772\u001b[0m                 )\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;31m# Look up the requested collection or package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error loading {info_or_id}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \"\"\"Return the ``Package`` or ``Collection`` record for the\n\u001b[1;32m   1005\u001b[0m         given item.\"\"\"\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;31m# Download the index file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         self._index = nltk.internals.ElementWrapper(\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0mElementTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'internals'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Ensure NLTK is correctly installed and restart the kernel\n",
        "nltk.download('wordnet')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxVTZ_d-CxBA"
      },
      "outputs": [],
      "source": [
        "url = \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
        "df = pd.read_csv(url, sep='\\t', compression='gzip', usecols=['review_body', 'star_rating'],on_bad_lines='skip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGdgRD_WCw-z"
      },
      "outputs": [],
      "source": [
        "#Ensure that star_rating contains only valid numeric values:\n",
        "\n",
        "# Convert star_rating to numeric, coercing errors to NaN\n",
        "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid star_rating values\n",
        "df = df.dropna(subset=['star_rating'])\n",
        "\n",
        "# Convert star_rating to integer (if required)\n",
        "df['star_rating'] = df['star_rating'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF2mnWdeCw8d"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUsq6bw6Cw6N"
      },
      "outputs": [],
      "source": [
        "balanced_df = df.groupby('star_rating').apply(lambda x: x.sample(n=50000, random_state=42)).reset_index(drop=True)\n",
        "balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xre8D6qmCw3x"
      },
      "outputs": [],
      "source": [
        "def map_sentiment(rating):\n",
        "    if rating > 3:\n",
        "        return 1  # Positive\n",
        "    elif rating < 3:\n",
        "        return 2  # Negative\n",
        "    else:\n",
        "        return 3  # Neutral\n",
        "\n",
        "balanced_df['sentiment'] = balanced_df['star_rating'].apply(map_sentiment)\n",
        "orig_df = balanced_df.copy()\n",
        "balanced_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRSKQ8SYCw1c"
      },
      "outputs": [],
      "source": [
        "#train_df, test_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['sentiment'], random_state=42)\n",
        "print(orig_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKJiwEj1Cwy8"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G0iDHFtCwwk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Download NLTK tokenizer\n",
        "nltk.download('punkt')\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0kCvpZOCwt_"
      },
      "outputs": [],
      "source": [
        "# Load pretrained Word2Vec model (Google News 300)\n",
        "wv = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Check the vector representation of a word\n",
        "#print(wv[\"excellent\"])  # A 300-dimensional vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiijCdl5Cwrw"
      },
      "outputs": [],
      "source": [
        "#check semantic similarities\n",
        "result = wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n",
        "print(\"King - Man + Woman =\", result[0][0])  # Expected output: 'queen'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m5V4gh6CwpG"
      },
      "outputs": [],
      "source": [
        "#similarities between words\n",
        "similarity = wv.similarity(\"excellent\", \"outstanding\")\n",
        "print(\"Similarity between 'excellent' and 'outstanding':\", similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OELIuvwtfvg8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "#tokenize your dataset\n",
        "# Tokenize the review text\n",
        "balanced_df[\"tokens\"] = balanced_df[\"review_body\"].astype(str).apply(word_tokenize)\n",
        "# Show some tokenized reviews\n",
        "print(balanced_df[\"tokens\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obYfpuUbfvey"
      },
      "outputs": [],
      "source": [
        "#converting to word embeddings\n",
        "'''\n",
        "def get_word2vec_embedding(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)  # 300D vector\n",
        "\n",
        "# Applying to dataset using the pretrained Word2Vec model\n",
        "balanced_df[\"embedding\"] = balanced_df[\"tokens\"].apply(lambda x: get_word2vec_embedding(x, wv))'''\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pretrained Word2Vec model\n",
        "wv = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Function to get embeddings for each word in a review\n",
        "def get_word_embeddings(tokens, model):\n",
        "    return [model[word] for word in tokens if word in model]  # List of word embeddings\n",
        "\n",
        "# Load dataset (assume it's preprocessed and tokenized)\n",
        "#df = pd.read_pickle(\"amazon_reviews.pkl\")  # Replace with actual dataset file\n",
        "\n",
        "# Apply function to dataset\n",
        "balanced_df[\"word_embeddings\"] = balanced_df[\"tokens\"].apply(lambda x: get_word_embeddings(x, wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxQtvu02r9A6"
      },
      "source": [
        "###(b) Training Word2Vec model on our own datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpTPVXy8fvZ9"
      },
      "outputs": [],
      "source": [
        "# Training my custom Word2Vec model\n",
        "custom_w2v = Word2Vec(\n",
        "    sentences=balanced_df[\"tokens\"],  # List of tokenized reviews\n",
        "    vector_size=300,         # Word embedding size\n",
        "    window=11,               # Context window size\n",
        "    min_count=10,            # Minimum word frequency threshold\n",
        "    workers=4                # Use multiple CPU cores for efficiency\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "custom_w2v.save(\"custom_word2vec.model\")\n",
        "\n",
        "# Example: Check words most similar to \"excellent\"\n",
        "print(custom_w2v.wv.most_similar(\"excellent\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jBH2L64fvXh"
      },
      "outputs": [],
      "source": [
        "result_custom = custom_w2v.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n",
        "print(\"Custom Word2Vec: King - Man + Woman =\", result_custom[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJzAAk5wfvVR"
      },
      "outputs": [],
      "source": [
        "similarity_custom = custom_w2v.wv.similarity(\"excellent\", \"outstanding\")\n",
        "print(\"Custom Word2Vec similarity (excellent vs outstanding):\", similarity_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsBFSX37GBj"
      },
      "source": [
        "**Comparison based on King - Man + Woman on above 2 models :**\n",
        "\n",
        "Google News Word2Vec : Returns \"Queen\"\n",
        "\n",
        "Custom Word2Vec (Amazon Reviews) : Does not return \"Queen\" as the dataset may lacks historical context.\n",
        "\n",
        "\n",
        "**Comparison based on Excellent vs Outstanding :**\n",
        "\n",
        "\n",
        "Google News Word2Vec : High similarity (~0.9)\n",
        "\n",
        "\n",
        "Custom Word2Vec (Amazon Reviews) :0.86168975 ( lower due to limited data.)\n",
        "\n",
        "\n",
        "**Based on Context Understanding**\n",
        "\n",
        "Google News Word2Vec : General knowledge\tBiased towards Amazon review words.\n",
        "\n",
        "Custom Word2Vec (Amazon Reviews): Biased towards Amazon review words.\n",
        "\n",
        "**Conclusion:-**\n",
        "\n",
        "Pretrained Word2Vec (Google News 300) performs better for general language tasks\n",
        "\n",
        "It was trained on a large-scale dataset (Google News corpus).\n",
        "It encodes rich semantic relationships between words.\n",
        "It correctly captures King - Man + Woman = Queen.\n",
        "\n",
        "Custom Word2Vec is more domain-specific (Amazon Reviews)\n",
        "\n",
        "It is trained on customer reviews, so it understands sentiment-related words well.\n",
        "But it lacks general world knowledge, making it weaker for analogy tasks.\n",
        "Example: It may not return \"Queen\" for King - Man + Woman, but it might capture sentiment-related similarities better (e.g., \"great\" âˆ¼ \"awesome\")\n",
        "\n",
        "**Best Choice?**\n",
        "\n",
        "If we need general word understanding, use then pretrained Word2Vec.\n",
        "\n",
        "If we need domain-specific representations (Amazon reviews, legal, medical, etc.),  our own custom trained Word2V on reviews works better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvYK9d3c7noK"
      },
      "outputs": [],
      "source": [
        "print(custom_w2v.wv.most_similar(\"excellent\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3."
      ],
      "metadata": {
        "id": "bpSzGTecPXK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data cleaning and Data Preprocessing"
      ],
      "metadata": {
        "id": "p4VaoX1DPss4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = {\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"I would\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"i'm\": \"I am\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}"
      ],
      "metadata": {
        "id": "mbbASRplQuqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=orig_df.copy()\n",
        "df"
      ],
      "metadata": {
        "id": "aJBwuBCjxS4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from locale import D_FMT\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "import re\n",
        "#from spellchecker import SpellChecker\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Initialize spell checker\n",
        "#spell = SpellChecker()\n",
        "\n",
        "# Function to filter misspelled words\n",
        "def remove_misspelled_words(text):\n",
        "    if isinstance(text, str):\n",
        "        # Split text into words\n",
        "        words = text.split()\n",
        "        # Check each word and retain it only if it's correctly spelled\n",
        "        corrected_words = [word for word in words if word in spell]\n",
        "        return \" \".join(corrected_words)\n",
        "    return \"\"\n",
        "\n",
        "# Download the English words corpus (only required once)\n",
        "nltk.download('words')\n",
        "\n",
        "# Get the set of valid English words\n",
        "english_words = set(words.words())\n",
        "\n",
        "# Function to filter non-English words from a string\n",
        "def filter_non_english_words(text):\n",
        "    if isinstance(text, str):\n",
        "        # Split text into words, filter non-English words, and rejoin\n",
        "        filtered_words = [word for word in text.split() if word.lower() in english_words]\n",
        "        return \" \".join(filtered_words)\n",
        "    return \"\"\n",
        "\n",
        "def expand_contractions(text, contractions_map):\n",
        "    if isinstance(text, str):\n",
        "        pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_map.keys()) + r')\\b')\n",
        "        return pattern.sub(lambda x: contractions_map[x.group()], text)\n",
        "    return text\n",
        "\n",
        "def preprocess_reviews(df):\n",
        "    # Step 1: Convert all reviews to lowercase\n",
        "    df['cleaned_review'] = df['review_body'].str.lower()\n",
        "\n",
        "    # Step 2: Remove HTML tags and URLs\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'<.*?>', '', x) if isinstance(x, str) else '')\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'http\\S+', '', x) if isinstance(x, str) else '')\n",
        "\n",
        "    # Step 3: Expand contractions (e.g., can't -> can not)\n",
        "    #df['cleaned_review'] = df['cleaned_review'].apply(lambda x: contractions.fix(x))\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: expand_contractions(x, contractions_dict))\n",
        "\n",
        "\n",
        "    # Step 4: Separate concatenated words (e.g., slot...can't -> slot can't)\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'(\\w)(\\'\\w)', r'\\1 \\2', x))  # Insert space between word and contraction\n",
        "\n",
        "    # Step 5: Replace multiple dots (two or more) with a space\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'\\.{2,}', ' ', x))\n",
        "\n",
        "    # Step 6: Replace non-alphabetical characters (except for apostrophes and spaces) with a space\n",
        "    #df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'[^\\w\\s\\'-]', ' ', x))  # Keep alphabet, spaces, apostrophes, and hyphens\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s\\']', ' ', x))  # Keep only alphabet, spaces, and apostrophes\n",
        "    # Step 7: Remove extra spaces (e.g., from replacing punctuation)\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "    # Step 8:  Apply the function to remove misspelled words\n",
        "    df['cleaned_review'] = df['cleaned_review'].apply(filter_non_english_words)\n",
        "    #df['cleaned_review'] = df['cleaned_review'].apply(remove_misspelled_words)\n",
        "    return df\n",
        "\n",
        "def average_length(df, column='review_body'):\n",
        "    # Replace non-string values with empty string\n",
        "    df[column] = df[column].fillna('').astype(str)\n",
        "\n",
        "    # Calculate average length\n",
        "    lengths = df[column].apply(len)\n",
        "    return lengths.mean()\n",
        "\n",
        "\n",
        "\n",
        "# Calculate average lengths before preprocessing\n",
        "avg_length_before = average_length(df, column='review_body')\n",
        "\n",
        "# Preprocess the reviews\n",
        "reduced_df = preprocess_reviews(df)\n",
        "\n",
        "# Calculate average lengths after preprocessing\n",
        "avg_length_after = average_length(df, column='cleaned_review')\n",
        "\n",
        "# Output average lengths before and after\n",
        "print(f\"Average length before Data Cleaning: {avg_length_before}, Average length after Data Cleaning: {avg_length_after}\")\n",
        "\n",
        "\n",
        "\n",
        "#Average length before cleaning: 319.694505\n",
        "#Average length after cleaning: 260.67447"
      ],
      "metadata": {
        "id": "ugrCxBO5PT4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize the lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to get the WordNet POS tag from the NLTK POS tag\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('VB'):  # Verbs\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('JJ'):  # Adjectives\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('NN'):  # Nouns\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('RB'):  # Adverbs\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if no match\n",
        "\n",
        "# Function to lemmatize text with POS tagging\n",
        "def lemmatize_text(text):\n",
        "    # Tokenize the sentence and POS tag each token\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Lemmatize using POS tags and filter out stopwords\n",
        "    lemmatized_words = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        for word, tag in tagged_tokens if word.lower() not in stop_words\n",
        "    ]\n",
        "\n",
        "    # Return the lemmatized sentence as a string\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to your dataframe column\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "amiUYYejPUZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average length before data preprocessing: {avg_length_after}, Average length after data preprocessing: {average_length(reduced_df, column='cleaned_review')}\")\n"
      ],
      "metadata": {
        "id": "aZntVr5gG3Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3classes = df.copy()\n",
        "# Labels: Sentiment classification (Class 1 = Positive, Class 2 = Negative)\n",
        "df = df[df[\"sentiment\"].isin([1, 2])]  # Ignore Class 3 (Neutral)"
      ],
      "metadata": {
        "id": "d4uhuPnB49IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_3classes.shape"
      ],
      "metadata": {
        "id": "FTP3Czsf49BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['sentiment'] == 5]\n",
        "print(filtered_df)"
      ],
      "metadata": {
        "id": "LWVUX-jQ5L06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Computing Word2Vec Embeddings from Google and Custom\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained Word2Vec (Google News 300)\n",
        "wv_google = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Load custom-trained Word2Vec\n",
        "custom_w2v = Word2Vec.load(\"custom_word2vec.model\")\n",
        "\n",
        "# Function to compute average word embedding for a review\n",
        "def get_avg_embedding(tokens, model):\n",
        "    vectors = [model[word] for word in tokens.split() if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)  # Return zero vector if no valid words\n",
        "\n",
        "# Compute embeddings for both models\n",
        "df[\"embedding_google\"] = df[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, wv_google))\n",
        "df[\"embedding_custom\"] = df[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, custom_w2v.wv))"
      ],
      "metadata": {
        "id": "tqNR_GYqPUWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=\"uiuiu hjhj jhj\"\n",
        "for word in s.split():\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "BBrHIBBdSPlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Tf-idf feature extraction\n",
        "newdf= reduced_df\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,  # Limit to top 5000 terms (optional)\n",
        "    stop_words='english',  # Remove common stop words\n",
        "    #ngram_range=(1,2)  # Use unigrams and bigrams\n",
        ")\n",
        "# Fit the vectorizer and transform the data\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_review'])"
      ],
      "metadata": {
        "id": "rV6HR2J7bdHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_features"
      ],
      "metadata": {
        "id": "d0RjKjhV4hOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train - Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train_google, X_test_google, y_train, y_test = train_test_split(\n",
        "    list(df[\"embedding_google\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_custom, X_test_custom, _, _ = train_test_split(\n",
        "    list(df[\"embedding_custom\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(\n",
        "    tfidf_features, df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")"
      ],
      "metadata": {
        "id": "BqqFtCYQbdED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train_google))  # <class 'list'>\n",
        "print(type(X_train_google[0]))  # <class 'numpy.ndarray'>\n",
        "print(X_train_google[0].shape)  # (300,)"
      ],
      "metadata": {
        "id": "-fmSmv1_9B7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train_custom))  # <class 'list'>\n",
        "print(type(X_train_custom[0]))  # <class 'numpy.ndarray'>\n",
        "print(X_train_custom[0].shape)  # (300,)"
      ],
      "metadata": {
        "id": "lMbRHSkU9Yvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perceptron training\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Training metrics\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    train_precision = precision_score(y_train, y_train_pred, average=\"binary\")\n",
        "    train_recall = recall_score(y_train, y_train_pred, average=\"binary\")\n",
        "    train_f1 = f1_score(y_train, y_train_pred, average=\"binary\")\n",
        "\n",
        "    # Test metrics\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred, average=\"binary\")\n",
        "    test_recall = recall_score(y_test, y_test_pred, average=\"binary\")\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average=\"binary\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nğŸ“Œ {model_name} Results:\")\n",
        "    print(f\"âœ… Training Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"âœ… Training Precision: {train_precision:.4f} | Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"âœ… Training Recall: {train_recall:.4f} | Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"âœ… Training F1 Score: {train_f1:.4f} | Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "    #return {\n",
        "    #    \"train_acc\": train_acc, \"test_acc\": test_acc,\n",
        "    #    \"train_precision\": train_precision, \"test_precision\": test_precision,\n",
        "    #    \"train_recall\": train_recall, \"test_recall\": test_recall,\n",
        "    #    \"train_f1\": train_f1, \"test_f1\": test_f1\n",
        "    #}\n",
        "\n",
        "### **1ï¸âƒ£ Perceptron Models**\n",
        "print(\"\\nğŸ”¹ Training Perceptron Models...\")\n",
        "perc_google = Perceptron()\n",
        "perc_google.fit(X_train_google, y_train)\n",
        "evaluate_model(perc_google, X_train_google, X_test_google, y_train, y_test, \"Perceptron (Google News W2V)\")\n",
        "\n",
        "perc_custom = Perceptron()\n",
        "perc_custom.fit(X_train_custom, y_train)\n",
        "evaluate_model(perc_custom, X_train_custom, X_test_custom, y_train, y_test, \"Perceptron (Custom W2V)\")\n",
        "\n",
        "perc_tfidf = Perceptron()\n",
        "perc_tfidf.fit(X_train_tfidf, y_train)\n",
        "evaluate_model(perc_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Perceptron (TF-IDF)\")\n",
        "\n",
        "\n",
        "### **2ï¸âƒ£ SVM Models**\n",
        "print(\"\\nğŸ”¹ Training SVM Models...\")\n",
        "svm_google = LinearSVC(random_state=42, max_iter=10000)\n",
        "svm_google.fit(X_train_google, y_train)\n",
        "evaluate_model(svm_google, X_train_google, X_test_google, y_train, y_test, \"SVM (Google News W2V)\")\n",
        "\n",
        "svm_custom = LinearSVC(random_state=42, max_iter=10000)\n",
        "svm_custom.fit(X_train_custom, y_train)\n",
        "evaluate_model(svm_custom, X_train_custom, X_test_custom, y_train, y_test, \"SVM (Custom W2V)\")\n",
        "\n",
        "svm_tfidf =LinearSVC(random_state=42, max_iter=10000)\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "evaluate_model(svm_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, \"SVM (TF-IDF)\")\n"
      ],
      "metadata": {
        "id": "NUMJvjoG--CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF remains the best performing feature type for both Perceptron and SVM, achieving: Best test accuracy (84.1%) with SVM.\n",
        "\n",
        "### SVM outperforms Perceptron consistently.\n",
        "\n",
        "### Custom Word2Vec benefits from domain-specific training but still lags behind TF-IDF."
      ],
      "metadata": {
        "id": "KJFxHK7G05ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "5F9Hw_Or-96F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skhNpLe--Lnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_3classes.copy()\n",
        "df = df[df[\"sentiment\"].isin([1, 2])]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "### Computing Word2Vec Embeddings from Google and Custom\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load pretrained Word2Vec (Google News 300)\n",
        "wv_google = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Load custom-trained Word2Vec\n",
        "custom_w2v = Word2Vec.load(\"custom_word2vec.model\")\n",
        "\n",
        "# Function to compute average word embedding for a review\n",
        "def get_avg_embedding(tokens, model):\n",
        "    vectors = [model[word] for word in tokens.split() if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)  # Return zero vector if no valid words\n",
        "\n",
        "# Compute embeddings for both models\n",
        "df[\"embedding_google\"] = df[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, wv_google))\n",
        "df[\"embedding_custom\"] = df[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, custom_w2v.wv))\n",
        "\n"
      ],
      "metadata": {
        "id": "9Cww3jix35CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split (80% training, 20% testing)\n",
        "X_train_google, X_test_google, y_train, y_test = train_test_split(\n",
        "    list(df[\"embedding_google\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_custom, X_test_custom, _, _ = train_test_split(\n",
        "    list(df[\"embedding_custom\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "#For ternary :\n",
        "df_3classes[\"embedding_google\"] = df_3classes[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, wv_google))\n",
        "df_3classes[\"embedding_custom\"] = df_3classes[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, custom_w2v.wv))\n",
        "\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train_google_ternary, X_test_google_ternary, y_train_ter, y_test_ter = train_test_split(\n",
        "    list(df_3classes[\"embedding_google\"]), df_3classes[\"sentiment\"], test_size=0.2, random_state=42, stratify=df_3classes[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_custom_ternary, X_test_custom_ternary, _, _ = train_test_split(\n",
        "    list(df_3classes[\"embedding_custom\"]), df_3classes[\"sentiment\"], test_size=0.2, random_state=42, stratify=df_3classes[\"sentiment\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "sBhmtPm1_F-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_binary = np.array(y_train.replace({1: 1, 2: 0}))\n",
        "y_test_binary = np.array(y_test.replace({1: 1, 2: 0}))"
      ],
      "metadata": {
        "id": "ht4ATVV868KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ternary = np.array(y_train_ter .replace({1: 0, 2: 1,3:2}))\n",
        "y_test_ternary = np.array(y_test_ter.replace({1: 0, 2: 1,3:2}))"
      ],
      "metadata": {
        "id": "9S6zqD_09kit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a Feedforward MLP with two hidden layers (50 & 10 nodes).\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 50)  # First hidden layer (50 neurons)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(50, 10)  # Second hidden layer (10 neurons)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(10, num_classes)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "L3M7YR9G3449"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, X_test, y_test, num_classes, epochs=250, lr=0.001):\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Define model\n",
        "    model = MLP(input_size=300, num_classes=num_classes)\n",
        "    criterion = nn.CrossEntropyLoss()  # Cross-entropy loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test)\n",
        "        _, predicted = torch.max(test_outputs, 1)\n",
        "        accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "BOykaGpa6D7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Binary Classification Models:\n",
        "print(\"Binary Classification (Positive vs Negative)\")\n",
        "\n",
        "# Train on Google Word2Vec\n",
        "acc_binary_google = train_model(X_train_google, y_train_binary, X_test_google, y_test_binary, num_classes=2)\n",
        "print()\n",
        "print(f\"âœ… Google W2V Accuracy (Binary): {acc_binary_google:.4f}\")\n",
        "print()\n",
        "# Train on Custom Word2Vec\n",
        "acc_binary_custom = train_model(X_train_custom, y_train_binary, X_test_custom, y_test_binary, num_classes=2)\n",
        "print(f\"âœ… Custom W2V Accuracy (Binary): {acc_binary_custom:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EebwiMcE6D4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ternary Classification (Positive, Negative, Neutral)\")\n",
        "#X_train_google_ternary, X_test_google_ternary\n",
        "#X_train_custom_ternary, X_test_custom_ternary\n",
        "# Train on Google Word2Vec\n",
        "acc_ternary_google = train_model(X_train_google_ternary, y_train_ternary, X_test_google_ternary, y_test_ternary, num_classes=3)\n",
        "print(f\"âœ… Google W2V Accuracy (Ternary): {acc_ternary_google:.4f}\")\n",
        "\n",
        "# Train on Custom Word2Vec\n",
        "acc_ternary_custom = train_model(X_train_custom_ternary, y_train_ternary, X_test_custom_ternary, y_test_ternary, num_classes=3)\n",
        "print(f\"âœ… Custom W2V Accuracy (Ternary): {acc_ternary_custom:.4f}\")\n"
      ],
      "metadata": {
        "id": "YG063lWK6D2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "et0Qub-Y6DxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b)"
      ],
      "metadata": {
        "id": "in1gL3tQIwxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_w2v_vectors(tokens, model, num_vectors=10):\n",
        "    # Use model[word] directly for KeyedVectors (Google W2V) and model.wv[word] for Word2Vec models\n",
        "    vectors = [model[word] if isinstance(model, gensim.models.KeyedVectors) else model.wv[word] for word in tokens if word in model]\n",
        "\n",
        "    # If fewer than num_vectors words exist, pad with zero vectors\n",
        "    while len(vectors) < num_vectors:\n",
        "        vectors.append(np.zeros(model.vector_size))\n",
        "\n",
        "    # Take only the first num_vectors\n",
        "    vectors = vectors[:num_vectors]\n",
        "\n",
        "    # Flatten (concatenate) the vectors\n",
        "    return np.concatenate(vectors, axis=0)\n",
        "\n",
        "# Convert dataset to concatenated vectors\n",
        "X_google_concat = np.array([concat_w2v_vectors(tokens, wv_google) for tokens in df[\"cleaned_review\"]])\n",
        "X_custom_concat = np.array([concat_w2v_vectors(tokens, custom_w2v.wv) for tokens in df[\"cleaned_review\"]])\n",
        "\n"
      ],
      "metadata": {
        "id": "48J2GG4i6Duq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data Shape:\", X_google_concat.shape)"
      ],
      "metadata": {
        "id": "WWI1oOMlL86l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Similarly for Ternary , get the 3000 input vector size:\n",
        "# Convert dataset to concatenated vectors\n",
        "X_google_concat_tern = np.array([concat_w2v_vectors(tokens, wv_google) for tokens in df_3classes[\"cleaned_review\"]])\n",
        "X_custom_concat_tern = np.array([concat_w2v_vectors(tokens, custom_w2v.wv) for tokens in df_3classes[\"cleaned_review\"]])"
      ],
      "metadata": {
        "id": "UPM26uiRL84V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ternary training Data Shape:\", X_google_concat_tern.shape)"
      ],
      "metadata": {
        "id": "tdgk_Ml8L81x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_google_concat = X_google_concat.astype(np.float32)\n",
        "X_custom_concat = X_custom_concat.astype(np.float32)\n",
        "X_google_concat_tern = X_google_concat_tern.astype(np.float32)\n",
        "X_custom_concat_tern = X_custom_concat_tern.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "Mf0SSvV-YQ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Binary classification split (one at a time)\n",
        "X_train_google_concat_binary, X_test_google_concat_binary, y_train_google_concat_binary, y_test_google_concat_binary = train_test_split(\n",
        "    X_google_concat, df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "del X_google_concat  # Free memory\n",
        "\n",
        "X_train_custom_concat_binary, X_test_custom_concat_binary, y_train_custom_concat_binary, y_test_custom_concat_binary = train_test_split(\n",
        "    X_custom_concat, df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "del X_custom_concat  # Free memory\n",
        "\n",
        "# Ternary classification split\n",
        "X_train_google_concat_ternary, X_test_google_concat_ternary, y_train_google_concat_ternary, y_test_google_concat_ternary = train_test_split(\n",
        "    X_google_concat_tern, df_3classes['sentiment'], test_size=0.2, random_state=42, stratify=df_3classes['sentiment'])\n",
        "\n",
        "del X_google_concat_tern  # Free memory\n",
        "\n"
      ],
      "metadata": {
        "id": "-XRD4sQWYUta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_custom_concat_ternary, X_test_custom_concat_ternary, y_train_custom_concat_ternary, y_test_custom_concat_ternary = train_test_split(\n",
        "    X_custom_concat_tern, df_3classes['sentiment'], test_size=0.2, random_state=42, stratify=df_3classes['sentiment'])\n",
        "\n",
        "del X_custom_concat_tern  # Free memory\n"
      ],
      "metadata": {
        "id": "YbYYzAHEh-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#Splitting binary into train and test : 80-20\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Binary\n",
        "X_train_google_concat_binary, X_test_google_concat_binary, y_train_google_concat_binary, y_test_google_concat_binary = train_test_split(\n",
        "    X_google_concat, df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "X_train_custom_concat_binary, X_test_custom_concat_binary, y_train_custom_concat_binary, y_test_custom_concat_binary = train_test_split(\n",
        "    X_custom_concat, df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "#Ternary\n",
        "\n",
        "X_train_google_concat_ternary, X_test_google_concat_ternary, y_train_google_concat_ternary, y_test_google_concat_ternary = train_test_split(\n",
        "    X_google_concat_tern, df_3classes['sentiment'], test_size=0.2, random_state=42, stratify=df_3classes['sentiment'])\n",
        "\n",
        "X_train_custom_concat_ternary, X_test_custom_concat_ternary, y_train_custom_concat_ternary, y_test_custom_concat_ternary = train_test_split(\n",
        "    X_custom_concat_tern, df_3classes['sentiment'], test_size=0.2, random_state=42, stratify=df_3classes['sentiment'])\n",
        "'''"
      ],
      "metadata": {
        "id": "-DeatCdZL8zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eBNGUV2L8xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8HBnzlML8ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''X_train_google, X_test_google, y_train, y_test = train_test_split(\n",
        "    list(df[\"embedding_google\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_custom, X_test_custom, _, _ = train_test_split(\n",
        "    list(df[\"embedding_custom\"]), df[\"sentiment\"], test_size=0.2, random_state=42, stratify=df[\"sentiment\"]\n",
        ")\n",
        "\n",
        "#For ternary :\n",
        "df_3classes[\"embedding_google\"] = df_3classes[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, wv_google))\n",
        "df_3classes[\"embedding_custom\"] = df_3classes[\"cleaned_review\"].apply(lambda x: get_avg_embedding(x, custom_w2v.wv))\n",
        "\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train_google_ternary, X_test_google_ternary, y_train_ter, y_test_ter = train_test_split(\n",
        "    list(df_3classes[\"embedding_google\"]), df_3classes[\"sentiment\"], test_size=0.2, random_state=42, stratify=df_3classes[\"sentiment\"]\n",
        ")\n",
        "\n",
        "X_train_custom_ternary, X_test_custom_ternary, _, _ = train_test_split(\n",
        "    list(df_3classes[\"embedding_custom\"]), df_3classes[\"sentiment\"], test_size=0.2, random_state=42, stratify=df_3classes[\"sentiment\"]\n",
        ")'''"
      ],
      "metadata": {
        "id": "qDBByaZYG3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjeSVmHAG3cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6NwKRgwG3ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KePDbZlkG3YE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "authorship_tag": "ABX9TyNYp6bNRSreno0BT0DYOg3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}