{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKvCDdi2OkqJvFCYdkJ1JT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junaid537/pytorch/blob/main/finalHW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_qVoOBuDN7",
        "outputId": "69f7b711-ddfa-4943-824a-0360b04d8066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\""
      ],
      "metadata": {
        "id": "qh_BftaBsCOF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_bV7DC5jt4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\""
      ],
      "metadata": {
        "id": "KP8-EFVG8Jl8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Paths\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "# output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# # Optimized Hyperparameters\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0.2\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.01\n",
        "# MOMENTUM = 0.9\n",
        "# WEIGHT_DECAY = 5e-5\n",
        "# EPOCHS = 30\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Load dataset\n",
        "# def load_data(filename, is_test=False):\n",
        "#     sentences, labels = [], []\n",
        "#     with open(filename, 'r', encoding='utf-8') as f:\n",
        "#         sentence, label = [], []\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if not line:\n",
        "#                 if sentence:\n",
        "#                     sentences.append(sentence)\n",
        "#                     labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 continue\n",
        "#             parts = line.split()\n",
        "#             if len(parts) < 2:\n",
        "#                 continue\n",
        "#             sentence.append(parts[1])\n",
        "#             if not is_test:\n",
        "#                 label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "#             else:\n",
        "#                 label.append(\"O\")\n",
        "#     if sentence:\n",
        "#         sentences.append(sentence)\n",
        "#         labels.append(label)\n",
        "#     return sentences, labels\n",
        "\n",
        "# # Load train, dev, and test data\n",
        "# train_sentences, train_labels = load_data(train_file)\n",
        "# dev_sentences, dev_labels = load_data(dev_file)\n",
        "# test_sentences, _ = load_data(test_file, is_test=True)  # No labels for test set\n",
        "\n",
        "# # Build vocabulary\n",
        "# word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "# tag_to_ix = {}\n",
        "# for sentence in train_sentences:\n",
        "#     for word in sentence:\n",
        "#         if word not in word_to_ix:\n",
        "#             word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# for labels in train_labels:\n",
        "#     for tag in labels:\n",
        "#         if tag not in tag_to_ix:\n",
        "#             tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# # Dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "#         self.sentences = sentences\n",
        "#         self.labels = labels\n",
        "#         self.word_to_ix = word_to_ix\n",
        "#         self.tag_to_ix = tag_to_ix\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sentence = self.sentences[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "#         label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to 0 for unknown tags\n",
        "#         return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# # Custom collate function\n",
        "# def collate_fn(batch):\n",
        "#     words, labels = zip(*batch)\n",
        "#     words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "#     labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "#     return words_padded, labels_padded\n",
        "\n",
        "# # Create datasets and loaders\n",
        "# train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "# dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "# test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)  # Dummy labels for test\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)  # FIXED: Now test_loader is defined!\n",
        "\n",
        "# # Define BLSTM Model\n",
        "# class BLSTM_NER(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size):\n",
        "#         super(BLSTM_NER, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "#                             bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.fc(x)\n",
        "#         x = self.elu(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, loss, optimizer, and scheduler\n",
        "# model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for words, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(words)\n",
        "\n",
        "#         # Reshape for loss computation\n",
        "#         outputs = outputs.view(-1, outputs.shape[-1])\n",
        "#         labels = labels.view(-1).long()\n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     scheduler.step()\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# # Save model\n",
        "# torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "# print(\"Model saved!\")\n",
        "\n",
        "# # Evaluation function\n",
        "# def evaluate(model, data_loader, sentences, output_file):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     batch_offset = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for words, _ in data_loader:\n",
        "#             outputs = model(words)\n",
        "#             outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "#             for batch_idx, preds in enumerate(outputs):\n",
        "#                 sentence = sentences[batch_offset]\n",
        "#                 for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "#                     predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "#                 predictions.append(\"\\n\")\n",
        "#                 batch_offset += 1\n",
        "\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.writelines(predictions)\n",
        "\n",
        "#     print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# # Evaluate on dev set\n",
        "# evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# # Evaluate on test set\n",
        "# evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "# print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "id": "rT7_IYhlVJW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vishal\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.25  # Adjusted dropout for better precision-recall balance\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 48  # Adjusted batch size for better updates\n",
        "LEARNING_RATE = 0.007\n",
        "MOMENTUM = 0.99\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS = 20\n",
        "STEP_SIZE = 7  # Learning rate scheduler step size\n",
        "GAMMA = 0.3  # Learning rate decay factor\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary\n",
        "word_to_ix = {\"PAD\": 0, \"UNK\": 1}\n",
        "tag_to_ix = {}\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]  # Default to UNK\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to O-tag\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "\n",
        "        # Reshape for loss computation\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # ✅ Apply gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "    print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# Evaluate on dev set\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5BJNEkbYjNw",
        "outputId": "9cb6b032-a389-4684-f27c-9094e6a55dc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 280.5661, LR: 0.007\n",
            "Epoch 2/20, Loss: 191.2348, LR: 0.007\n",
            "Epoch 3/20, Loss: 146.4206, LR: 0.007\n",
            "Epoch 4/20, Loss: 114.7212, LR: 0.007\n",
            "Epoch 5/20, Loss: 90.5919, LR: 0.007\n",
            "Epoch 6/20, Loss: 72.7196, LR: 0.007\n",
            "Epoch 7/20, Loss: 57.7537, LR: 0.0021\n",
            "Epoch 8/20, Loss: 42.7167, LR: 0.0021\n",
            "Epoch 9/20, Loss: 37.0447, LR: 0.0021\n",
            "Epoch 10/20, Loss: 33.3619, LR: 0.0021\n",
            "Epoch 11/20, Loss: 30.7095, LR: 0.0021\n",
            "Epoch 12/20, Loss: 27.5965, LR: 0.0021\n",
            "Epoch 13/20, Loss: 25.1166, LR: 0.0021\n",
            "Epoch 14/20, Loss: 22.4446, LR: 0.0006299999999999999\n",
            "Epoch 15/20, Loss: 19.6195, LR: 0.0006299999999999999\n",
            "Epoch 16/20, Loss: 18.6696, LR: 0.0006299999999999999\n",
            "Epoch 17/20, Loss: 18.0539, LR: 0.0006299999999999999\n",
            "Epoch 18/20, Loss: 17.4832, LR: 0.0006299999999999999\n",
            "Epoch 19/20, Loss: 17.0238, LR: 0.0006299999999999999\n",
            "Epoch 20/20, Loss: 16.3528, LR: 0.0006299999999999999\n",
            "Model saved!\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/dev.out\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/test.out\n",
            "Training completed. Model and outputs saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Optimized Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.12  # Lower dropout to improve recall\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01  # More stable learning\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-4  # Less regularization to prevent underfitting\n",
        "EPOCHS = 30  # Train longer for better F1\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)  # No labels for test set\n",
        "\n",
        "# Build vocabulary\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to 0 for unknown tags\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)  # Dummy labels for test\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# ✅ Learning Rate Scheduler (Reduce LR every 10 epochs instead of 5)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.005, max_lr=0.015, step_size_up=5, mode='triangular2')\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "\n",
        "        # Reshape for loss computation\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # ✅ Apply gradient clipping here\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Update learning rate if using a scheduler\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f},LR: {scheduler.get_last_lr()[0]} \")#LR: {scheduler.get_last_lr()[0]}\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "    print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# Evaluate on dev set\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xEQooxdVJU7",
        "outputId": "178512e4-48c6-42af-cf18-73d06bb8d201"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.12 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 386.8792,LR: 0.007000000000000001 \n",
            "Epoch 2/30, Loss: 330.6033,LR: 0.008999999999999998 \n",
            "Epoch 3/30, Loss: 313.8227,LR: 0.011 \n",
            "Epoch 4/30, Loss: 297.7265,LR: 0.012999999999999998 \n",
            "Epoch 5/30, Loss: 278.0630,LR: 0.015 \n",
            "Epoch 6/30, Loss: 255.4876,LR: 0.012999999999999998 \n",
            "Epoch 7/30, Loss: 234.8463,LR: 0.011 \n",
            "Epoch 8/30, Loss: 217.7632,LR: 0.008999999999999998 \n",
            "Epoch 9/30, Loss: 203.4266,LR: 0.007000000000000001 \n",
            "Epoch 10/30, Loss: 192.4864,LR: 0.005 \n",
            "Epoch 11/30, Loss: 183.6323,LR: 0.006000000000000001 \n",
            "Epoch 12/30, Loss: 176.0340,LR: 0.007000000000000001 \n",
            "Epoch 13/30, Loss: 168.9199,LR: 0.007999999999999998 \n",
            "Epoch 14/30, Loss: 162.0543,LR: 0.008999999999999998 \n",
            "Epoch 15/30, Loss: 155.1086,LR: 0.009999999999999998 \n",
            "Epoch 16/30, Loss: 148.4330,LR: 0.008999999999999998 \n",
            "Epoch 17/30, Loss: 141.2586,LR: 0.007999999999999998 \n",
            "Epoch 18/30, Loss: 134.9777,LR: 0.007000000000000001 \n",
            "Epoch 19/30, Loss: 128.9794,LR: 0.006000000000000001 \n",
            "Epoch 20/30, Loss: 123.7050,LR: 0.005 \n",
            "Epoch 21/30, Loss: 119.2117,LR: 0.0055000000000000005 \n",
            "Epoch 22/30, Loss: 114.5980,LR: 0.006000000000000001 \n",
            "Epoch 23/30, Loss: 110.4801,LR: 0.006499999999999999 \n",
            "Epoch 24/30, Loss: 106.1149,LR: 0.006999999999999999 \n",
            "Epoch 25/30, Loss: 101.7577,LR: 0.0075 \n",
            "Epoch 26/30, Loss: 96.9141,LR: 0.006999999999999999 \n",
            "Epoch 27/30, Loss: 92.4798,LR: 0.006499999999999999 \n",
            "Epoch 28/30, Loss: 88.0925,LR: 0.006000000000000001 \n",
            "Epoch 29/30, Loss: 84.0656,LR: 0.0055000000000000005 \n",
            "Epoch 30/30, Loss: 80.2392,LR: 0.005 \n",
            "Model saved!\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/dev.out\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/test.out\n",
            "Training completed. Model and outputs saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code with 70 F1\n",
        "#using SGD\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# import os\n",
        "\n",
        "# # Paths\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\"\n",
        "# test_file = \"/content/drive/MyDrive/dataHW4csci544/data/test\"\n",
        "# output_dir = \"/content/drive/MyDrive/dataHW4csci544/\"\n",
        "\n",
        "# # Hyperparameters\n",
        "# # EMBEDDING_DIM = 100\n",
        "# # HIDDEN_DIM = 256\n",
        "# # LSTM_LAYERS = 1\n",
        "# # DROPOUT = 0.33\n",
        "# # LINEAR_OUT_DIM = 128\n",
        "# # BATCH_SIZE = 32\n",
        "# # LEARNING_RATE = 0.1  # Adjusted for SGD\n",
        "# # MOMENTUM = 0.9  # Momentum for SGD\n",
        "# # WEIGHT_DECAY = 1e-4  # L2 Regularization\n",
        "# # EPOCHS = 10  # Increased for better training\n",
        "# EMBEDDING_DIM = 100\n",
        "# HIDDEN_DIM = 256\n",
        "# LSTM_LAYERS = 1\n",
        "# DROPOUT = 0.2\n",
        "# LINEAR_OUT_DIM = 128\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.01  # Adjusted for SGD\n",
        "# MOMENTUM = 0.9  # Momentum for SGD\n",
        "# WEIGHT_DECAY = 5e-5  # L2 Regularization\n",
        "# EPOCHS = 20  # Increased for better training\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Load dataset (Handles both train/dev and test files)\n",
        "# def load_data(filename, is_test=False):\n",
        "#     sentences, labels = [], []\n",
        "#     with open(filename, 'r', encoding='utf-8') as f:\n",
        "#         sentence, label = [], []\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if not line:\n",
        "#                 if sentence:\n",
        "#                     sentences.append(sentence)\n",
        "#                     labels.append(label)\n",
        "#                     sentence, label = [], []\n",
        "#                 continue\n",
        "#             parts = line.split()\n",
        "#             if len(parts) < 2:\n",
        "#                 continue  # Skip malformed lines\n",
        "#             sentence.append(parts[1])  # Word\n",
        "#             if not is_test:\n",
        "#                 label.append(parts[2] if len(parts) > 2 else \"O\")  # NER Tag\n",
        "#             else:\n",
        "#                 label.append(\"O\")  # Assign \"O\" for test set\n",
        "#     if sentence:\n",
        "#         sentences.append(sentence)\n",
        "#         labels.append(label)\n",
        "#     return sentences, labels\n",
        "\n",
        "# # Load train and dev data\n",
        "# train_sentences, train_labels = load_data(train_file)\n",
        "# dev_sentences, dev_labels = load_data(dev_file)\n",
        "\n",
        "# # Build vocabulary\n",
        "# word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "# tag_to_ix = {}\n",
        "# for sentence in train_sentences:\n",
        "#     for word in sentence:\n",
        "#         if word not in word_to_ix:\n",
        "#             word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# for labels in train_labels:\n",
        "#     for tag in labels:\n",
        "#         if tag not in tag_to_ix:\n",
        "#             tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# # Dataset class\n",
        "# class NERDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "#         self.sentences = sentences\n",
        "#         self.labels = labels\n",
        "#         self.word_to_ix = word_to_ix\n",
        "#         self.tag_to_ix = tag_to_ix\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sentences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sentence = self.sentences[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "#         label_indices = [self.tag_to_ix[l] for l in label]\n",
        "#         return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# # Custom collate function for dynamic padding\n",
        "# def collate_fn(batch):\n",
        "#     words, labels = zip(*batch)\n",
        "#     words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "#     labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "#     return words_padded, labels_padded\n",
        "\n",
        "# # Create datasets and loaders\n",
        "# train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "# dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# # Define BLSTM Model\n",
        "# class BLSTM_NER(nn.Module):\n",
        "#     def __init__(self, vocab_size, tagset_size):\n",
        "#         super(BLSTM_NER, self).__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "#         self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "#                             bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "#         self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.embedding(x)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = self.fc(x)\n",
        "#         x = self.elu(x)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "# # Initialize model, loss, and optimizer (Using SGD Now)\n",
        "# model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# # **Using SGD Instead of Adam**\n",
        "# #optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "# # Learning Rate Scheduler (Reduce LR every 5 epochs)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "# # Training loop (Fixed)\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for words, labels in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(words)  # [batch_size, max_seq_len, num_classes]\n",
        "\n",
        "#         # Reshape for loss computation\n",
        "#         outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * max_seq_len, num_classes]\n",
        "#         labels = labels.view(-1).long()  # [batch_size * max_seq_len]\n",
        "\n",
        "#         # Ensure loss ignores padding labels (-1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# # Save model\n",
        "# torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "# print(\"Model saved!\")\n",
        "\n",
        "# # Evaluation function (Fixed Indexing)\n",
        "# def evaluate(model, data_loader, sentences, output_file):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     batch_offset = 0  # Ensure correct sentence order\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for words, _ in data_loader:\n",
        "#             outputs = model(words)\n",
        "#             outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "#             for batch_idx, preds in enumerate(outputs):\n",
        "#                 sentence = sentences[batch_offset]\n",
        "#                 for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "#                     predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "#                 predictions.append(\"\\n\")  # Ensure sentence separation\n",
        "#                 batch_offset += 1  # Move to next sentence\n",
        "\n",
        "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.writelines(predictions)\n",
        "\n",
        "#     print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# # Evaluate on dev set\n",
        "# evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# # Evaluate on test set\n",
        "# test_sentences, test_labels = load_data(test_file, is_test=True)\n",
        "# test_dataset = NERDataset(test_sentences, test_labels, word_to_ix, tag_to_ix)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "# evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "# print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "id": "lhApZjhCIRas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "5d8G2hjeJmLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b86ffe-89ed-4b01-c774-f54c3c2cb673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5393 phrases; correct: 3995.\n",
            "accuracy:  94.39%; precision:  74.08%; recall:  67.23%; FB1:  70.49\n",
            "              LOC: precision:  85.77%; recall:  72.51%; FB1:  78.58  1553\n",
            "             MISC: precision:  76.35%; recall:  65.84%; FB1:  70.70  795\n",
            "              ORG: precision:  65.00%; recall:  63.16%; FB1:  64.07  1303\n",
            "              PER: precision:  69.40%; recall:  65.64%; FB1:  67.47  1742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83rKGeO1ImW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0_4l-m0ImUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjZ9ZVOaImR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzqTRsNDdeOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHfNx6VXVeF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "# dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\""
      ],
      "metadata": {
        "id": "KmKPBh6kVeC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aZDL6_Y_vJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StULyBDB54Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#python eval.py -p /content/drive/MyDrive/dataHW4csci544/dev1.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "x1n9ojIj_k5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "kB5tFmNVqjf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_path = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"  # GloVe embeddings file\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.1\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.85\n",
        "WEIGHT_DECAY = 3e-5\n",
        "EPOCHS = 35\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract GloVe embeddings from .gz file\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with gzip.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary with GloVe embeddings\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "\n",
        "# Step 1: Create an embedding matrix\n",
        "embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix) + 50000, EMBEDDING_DIM))  # Extra space for new words\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# Assign GloVe vectors to words (handling case sensitivity)\n",
        "for word, idx in word_to_ix.items():\n",
        "    lowercase_word = word.lower()\n",
        "    if lowercase_word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[lowercase_word]\n",
        "    elif word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Map entity labels to indexes\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model with GloVe Embeddings\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_matrix):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix), embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.85, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm2.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# **Save predictions as dev2 and test2**\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "# Save outputs as dev2 and test2\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev2.out\"))\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test2.out\"))\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "ju13f8NS_k0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2e057d-b85d-4b03-a4ac-149eb09232fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 316.5207, LR: 0.01\n",
            "Epoch 2, Loss: 232.0206, LR: 0.01\n",
            "Epoch 3, Loss: 192.2139, LR: 0.01\n",
            "Epoch 4, Loss: 163.6787, LR: 0.01\n",
            "Epoch 5, Loss: 145.3240, LR: 0.01\n",
            "Epoch 6, Loss: 131.2901, LR: 0.01\n",
            "Epoch 7, Loss: 119.8484, LR: 0.01\n",
            "Epoch 8, Loss: 111.0587, LR: 0.01\n",
            "Epoch 9, Loss: 103.2982, LR: 0.01\n",
            "Epoch 10, Loss: 96.9492, LR: 0.008\n",
            "Epoch 11, Loss: 91.4502, LR: 0.008\n",
            "Epoch 12, Loss: 87.2833, LR: 0.008\n",
            "Epoch 13, Loss: 83.5890, LR: 0.008\n",
            "Epoch 14, Loss: 80.2770, LR: 0.008\n",
            "Epoch 15, Loss: 77.2407, LR: 0.008\n",
            "Epoch 16, Loss: 75.0093, LR: 0.008\n",
            "Epoch 17, Loss: 71.8924, LR: 0.008\n",
            "Epoch 18, Loss: 69.2763, LR: 0.008\n",
            "Epoch 19, Loss: 67.4815, LR: 0.008\n",
            "Epoch 20, Loss: 64.9579, LR: 0.0064\n",
            "Epoch 21, Loss: 63.2328, LR: 0.0064\n",
            "Epoch 22, Loss: 61.8117, LR: 0.0064\n",
            "Epoch 23, Loss: 60.1013, LR: 0.0064\n",
            "Epoch 24, Loss: 58.8562, LR: 0.0064\n",
            "Epoch 25, Loss: 57.3848, LR: 0.0064\n",
            "Epoch 26, Loss: 55.9593, LR: 0.0064\n",
            "Epoch 27, Loss: 55.1110, LR: 0.0064\n",
            "Epoch 28, Loss: 53.8550, LR: 0.0064\n",
            "Epoch 29, Loss: 52.7027, LR: 0.0064\n",
            "Epoch 30, Loss: 51.6905, LR: 0.00512\n",
            "Epoch 31, Loss: 50.7616, LR: 0.00512\n",
            "Epoch 32, Loss: 49.9726, LR: 0.00512\n",
            "Epoch 33, Loss: 49.0009, LR: 0.00512\n",
            "Epoch 34, Loss: 48.2148, LR: 0.00512\n",
            "Epoch 35, Loss: 47.7671, LR: 0.00512\n",
            "Model saved!\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OoPad5IqekA",
        "outputId": "13fb42fd-a7c2-453c-d548-6fb264b1013a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev2.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxu3Tujqqhj6",
        "outputId": "94d23149-f648-4771-e5cf-efdb6015ab3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5489 phrases; correct: 4428.\n",
            "accuracy:  95.51%; precision:  80.67%; recall:  74.52%; FB1:  77.47\n",
            "              LOC: precision:  88.42%; recall:  82.31%; FB1:  85.26  1710\n",
            "             MISC: precision:  78.25%; recall:  69.85%; FB1:  73.81  823\n",
            "              ORG: precision:  66.14%; recall:  64.95%; FB1:  65.54  1317\n",
            "              PER: precision:  85.48%; recall:  76.06%; FB1:  80.49  1639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/hw4csci544/eval.py\"\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(\"✅ File exists!\")\n",
        "else:\n",
        "    print(\"❌ File does NOT exist!\")"
      ],
      "metadata": {
        "id": "tuWx59ii_kue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549fb900-364b-4c7a-dc22-9f4b31c6c0eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdRTu8mG_krk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "nAn_h7tD3E7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8lMhX0KjWZYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b68162-53db-48b7-9e7c-3e2113ac7f1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_path = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100  # Word embedding dimension\n",
        "CHAR_EMBEDDING_DIM = 30  # Character embedding dimension\n",
        "HIDDEN_DIM = 256  # LSTM hidden dimension\n",
        "LSTM_LAYERS = 1  # Number of LSTM layers\n",
        "CNN_OUT_DIM = 50  # CNN output dimension\n",
        "CNN_KERNEL_SIZE = 3  # CNN kernel size\n",
        "DROPOUT = 0.1  # Dropout rate\n",
        "LINEAR_OUT_DIM = 128  # Linear layer output dimension\n",
        "BATCH_SIZE = 32  # Batch size\n",
        "LEARNING_RATE = 0.01  # Learning rate\n",
        "MOMENTUM = 0.85  # Momentum for SGD\n",
        "WEIGHT_DECAY = 3e-5  # Weight decay\n",
        "EPOCHS = 20  # Number of epochs\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with gzip.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary and character vocabulary\n",
        "word_to_ix = {\"\": 0, \"\": 1}\n",
        "char_to_ix = {\"\": 0, \"\": 1}\n",
        "tag_to_ix = {}\n",
        "\n",
        "# Create word and character vocabularies\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        for char in word:\n",
        "            if char not in char_to_ix:\n",
        "                char_to_ix[char] = len(char_to_ix)\n",
        "\n",
        "# Create embedding matrix for words\n",
        "embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix) + 50000, EMBEDDING_DIM))\n",
        "for word, idx in word_to_ix.items():\n",
        "    lowercase_word = word.lower()\n",
        "    if lowercase_word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[lowercase_word]\n",
        "    elif word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Map entity labels to indexes\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, char_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.char_to_ix = char_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        char_indices = [[self.char_to_ix.get(c, 1) for c in word] for word in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]\n",
        "        return torch.tensor(word_indices), char_indices, torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, chars, labels = zip(*batch)\n",
        "\n",
        "    # Pad word sequences\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad label sequences\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "\n",
        "    # Pad character sequences\n",
        "    max_word_len = max([max([len(word) for word in sentence]) for sentence in chars])  # Longest word in the batch\n",
        "    max_sentence_len = max([len(sentence) for sentence in chars])  # Longest sentence in the batch\n",
        "\n",
        "    # Initialize a tensor for padded character sequences\n",
        "    chars_padded = torch.zeros((len(chars), max_sentence_len, max_word_len), dtype=torch.long)\n",
        "\n",
        "    # Fill the tensor with character indices\n",
        "    for i, sentence in enumerate(chars):\n",
        "        for j, word in enumerate(sentence):\n",
        "            chars_padded[i, j, :len(word)] = torch.tensor(word, dtype=torch.long)\n",
        "\n",
        "    return words_padded, chars_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, char_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, char_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, char_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define LSTM-CNN Model\n",
        "class LSTM_CNN_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, char_vocab_size, tagset_size, embedding_matrix, char_embedding_dim=30):\n",
        "        super(LSTM_CNN_NER, self).__init__()\n",
        "        # Word embeddings\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        # Character embeddings\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=0)\n",
        "        # CNN for character-level features\n",
        "        self.cnn = nn.Conv1d(char_embedding_dim, CNN_OUT_DIM, CNN_KERNEL_SIZE, padding=1)\n",
        "        # BLSTM for word-level features\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM + CNN_OUT_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        # Output layer\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        # Word embeddings\n",
        "        word_embeds = self.word_embedding(words)\n",
        "        # Character embeddings\n",
        "        batch_size, seq_len, word_len = chars.size()\n",
        "        chars = chars.view(batch_size * seq_len, word_len)\n",
        "        char_embeds = self.char_embedding(chars)\n",
        "        char_embeds = char_embeds.permute(0, 2, 1)\n",
        "        # CNN for character-level features\n",
        "        char_features = self.cnn(char_embeds)\n",
        "        char_features, _ = torch.max(char_features, dim=2)\n",
        "        char_features = char_features.view(batch_size, seq_len, -1)\n",
        "        # Concatenate word and character features\n",
        "        combined_embeds = torch.cat((word_embeds, char_features), dim=2)\n",
        "        # BLSTM\n",
        "        lstm_out, _ = self.lstm(combined_embeds)\n",
        "        # Fully connected layer\n",
        "        fc_out = self.fc(lstm_out)\n",
        "        fc_out = self.elu(fc_out)\n",
        "        # Output layer\n",
        "        logits = self.classifier(fc_out)\n",
        "        return logits\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = LSTM_CNN_NER(len(word_to_ix), len(char_to_ix), len(tag_to_ix), embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, chars, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words, chars)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm3.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, chars, _ in data_loader:\n",
        "            outputs = model(words, chars)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "# Save outputs as dev3.out and test3.out\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev3.out\"))\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test3.out\"))\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIwYGy5YyTuF",
        "outputId": "18677997-5734-451b-8040-cbbb758a55ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 308.0103, LR: 0.01\n",
            "Epoch 2, Loss: 202.5471, LR: 0.01\n",
            "Epoch 3, Loss: 146.6911, LR: 0.01\n",
            "Epoch 4, Loss: 111.8269, LR: 0.01\n",
            "Epoch 5, Loss: 88.8739, LR: 0.01\n",
            "Epoch 6, Loss: 71.9349, LR: 0.01\n",
            "Epoch 7, Loss: 59.0237, LR: 0.01\n",
            "Epoch 8, Loss: 49.9647, LR: 0.01\n",
            "Epoch 9, Loss: 44.6148, LR: 0.01\n",
            "Epoch 10, Loss: 40.6931, LR: 0.008\n",
            "Model saved!\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev3.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNUyh228_er8",
        "outputId": "00b6d8ed-1954-44f1-8521-f0822850ec74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 6037 phrases; correct: 4674.\n",
            "accuracy:  96.49%; precision:  77.42%; recall:  78.66%; FB1:  78.04\n",
            "              LOC: precision:  88.55%; recall:  81.27%; FB1:  84.76  1686\n",
            "             MISC: precision:  59.09%; recall:  72.23%; FB1:  65.01  1127\n",
            "              ORG: precision:  66.11%; recall:  70.69%; FB1:  68.32  1434\n",
            "              PER: precision:  87.54%; recall:  85.07%; FB1:  86.29  1790\n"
          ]
        }
      ]
    }
  ]
}