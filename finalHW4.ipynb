{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM0h4+eZtuH71zT+5TIjJVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junaid537/pytorch/blob/main/finalHW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_qVoOBuDN7",
        "outputId": "b87bfd0a-188c-4e7f-af96-2df55e9c11c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\""
      ],
      "metadata": {
        "id": "qh_BftaBsCOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_bV7DC5jt4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"/content/drive/MyDrive/dataHW4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/dataHW4csci544/data/dev\""
      ],
      "metadata": {
        "id": "KP8-EFVG8Jl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.25  # Adjusted dropout for better precision-recall balance\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 48  # Adjusted batch size for better updates\n",
        "LEARNING_RATE = 0.007\n",
        "MOMENTUM = 0.99\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS = 20\n",
        "STEP_SIZE = 7  # Learning rate scheduler step size\n",
        "GAMMA = 0.3  # Learning rate decay factor\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary\n",
        "word_to_ix = {\"PAD\": 0, \"UNK\": 1}\n",
        "tag_to_ix = {}\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]  # Default to UNK\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]  # Default to O-tag\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix))\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "\n",
        "        # Reshape for loss computation\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # ✅ Apply gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm1.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "    print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# Evaluate on dev set\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev.out\"))\n",
        "\n",
        "# Evaluate on test set\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test.out\"))\n",
        "\n",
        "print(\"Training completed. Model and outputs saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5BJNEkbYjNw",
        "outputId": "2d3c1f8a-dec1-4bcd-fe2b-308197178eaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 279.8087, LR: 0.007\n",
            "Epoch 2/20, Loss: 185.5096, LR: 0.007\n",
            "Epoch 3/20, Loss: 142.2065, LR: 0.007\n",
            "Epoch 4/20, Loss: 110.7185, LR: 0.007\n",
            "Epoch 5/20, Loss: 87.2723, LR: 0.007\n",
            "Epoch 6/20, Loss: 69.5188, LR: 0.007\n",
            "Epoch 7/20, Loss: 54.3767, LR: 0.0021\n",
            "Epoch 8/20, Loss: 40.3082, LR: 0.0021\n",
            "Epoch 9/20, Loss: 35.5232, LR: 0.0021\n",
            "Epoch 10/20, Loss: 31.9133, LR: 0.0021\n",
            "Epoch 11/20, Loss: 29.2835, LR: 0.0021\n",
            "Epoch 12/20, Loss: 26.2779, LR: 0.0021\n",
            "Epoch 13/20, Loss: 23.8541, LR: 0.0021\n",
            "Epoch 14/20, Loss: 21.9110, LR: 0.0006299999999999999\n",
            "Epoch 15/20, Loss: 18.7126, LR: 0.0006299999999999999\n",
            "Epoch 16/20, Loss: 17.7542, LR: 0.0006299999999999999\n",
            "Epoch 17/20, Loss: 17.2455, LR: 0.0006299999999999999\n",
            "Epoch 18/20, Loss: 16.7165, LR: 0.0006299999999999999\n",
            "Epoch 19/20, Loss: 16.1416, LR: 0.0006299999999999999\n",
            "Epoch 20/20, Loss: 15.7052, LR: 0.0006299999999999999\n",
            "Model saved!\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/dev.out\n",
            "Saved predictions to /content/drive/MyDrive/hw4csci544/test.out\n",
            "Training completed. Model and outputs saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "5d8G2hjeJmLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7292a135-ad59-4a2f-ec8a-5e8a7e55dfbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5906 phrases; correct: 4186.\n",
            "accuracy:  94.28%; precision:  70.88%; recall:  70.45%; FB1:  70.66\n",
            "              LOC: precision:  84.65%; recall:  78.33%; FB1:  81.37  1700\n",
            "             MISC: precision:  79.22%; recall:  68.22%; FB1:  73.31  794\n",
            "              ORG: precision:  66.67%; recall:  59.21%; FB1:  62.72  1191\n",
            "              PER: precision:  59.61%; recall:  71.88%; FB1:  65.17  2221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83rKGeO1ImW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#python eval.py -p /content/drive/MyDrive/dataHW4csci544/dev1.out -g /content/drive/MyDrive/dataHW4csci544/data/dev\n"
      ],
      "metadata": {
        "id": "x1n9ojIj_k5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "kB5tFmNVqjf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_path = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"  # GloVe embeddings file\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.1\n",
        "LINEAR_OUT_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.02\n",
        "MOMENTUM = 0.99\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 15\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Extract GloVe embeddings from .gz file\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with gzip.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary with GloVe embeddings\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag_to_ix = {}\n",
        "\n",
        "# Step 1: Create an embedding matrix\n",
        "#embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix), EMBEDDING_DIM))# Extra space for new words\n",
        "embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix), EMBEDDING_DIM))\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# Assign GloVe vectors to words (handling case sensitivity)\n",
        "for word, idx in word_to_ix.items():\n",
        "    lowercase_word = word.lower()\n",
        "    if lowercase_word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[lowercase_word]\n",
        "    elif word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Map entity labels to indexes\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]\n",
        "        return torch.tensor(word_indices), torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, labels = zip(*batch)\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "    return words_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define BLSTM Model with GloVe Embeddings\n",
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_matrix):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = BLSTM_NER(len(word_to_ix), len(tag_to_ix), embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.85, nesterov=True, weight_decay=WEIGHT_DECAY)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm2.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# **Save predictions as dev2 and test2**\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, _ in data_loader:\n",
        "            outputs = model(words)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "# Save outputs as dev2 and test2\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev2.out\"))\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test2.out\"))\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "ju13f8NS_k0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1154d8d0-7c4d-4b08-fe33-21f12c411525"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 220.7482, LR: 0.02\n",
            "Epoch 2, Loss: 70.0817, LR: 0.02\n",
            "Epoch 3, Loss: 45.0103, LR: 0.02\n",
            "Epoch 4, Loss: 33.4547, LR: 0.01\n",
            "Epoch 5, Loss: 24.5399, LR: 0.01\n",
            "Epoch 6, Loss: 20.6272, LR: 0.01\n",
            "Epoch 7, Loss: 18.6101, LR: 0.01\n",
            "Epoch 8, Loss: 15.7267, LR: 0.005\n",
            "Epoch 9, Loss: 12.4048, LR: 0.005\n",
            "Epoch 10, Loss: 10.9942, LR: 0.005\n",
            "Epoch 11, Loss: 10.1961, LR: 0.005\n",
            "Epoch 12, Loss: 9.5262, LR: 0.0025\n",
            "Epoch 13, Loss: 8.1570, LR: 0.0025\n",
            "Epoch 14, Loss: 7.3447, LR: 0.0025\n",
            "Epoch 15, Loss: 6.9792, LR: 0.0025\n",
            "Model saved!\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OoPad5IqekA",
        "outputId": "13fb42fd-a7c2-453c-d548-6fb264b1013a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev2.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxu3Tujqqhj6",
        "outputId": "932515d6-407b-4b92-fa12-37674167e2cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5456 phrases; correct: 4880.\n",
            "accuracy:  96.92%; precision:  89.44%; recall:  82.13%; FB1:  85.63\n",
            "              LOC: precision:  93.82%; recall:  87.53%; FB1:  90.57  1714\n",
            "             MISC: precision:  87.20%; recall:  78.31%; FB1:  82.51  828\n",
            "              ORG: precision:  84.69%; recall:  78.37%; FB1:  81.41  1241\n",
            "              PER: precision:  89.60%; recall:  81.38%; FB1:  85.29  1673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/hw4csci544/eval.py\"\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(\"✅ File exists!\")\n",
        "else:\n",
        "    print(\"❌ File does NOT exist!\")"
      ],
      "metadata": {
        "id": "tuWx59ii_kue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549fb900-364b-4c7a-dc22-9f4b31c6c0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdRTu8mG_krk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "nAn_h7tD3E7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Paths\n",
        "train_file = \"/content/drive/MyDrive/hw4csci544/data/train\"\n",
        "dev_file = \"/content/drive/MyDrive/hw4csci544/data/dev\"\n",
        "test_file = \"/content/drive/MyDrive/hw4csci544/data/test\"\n",
        "glove_path = \"/content/drive/MyDrive/hw4csci544/glove.6B.100d.gz\"\n",
        "output_dir = \"/content/drive/MyDrive/hw4csci544/\"\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100  # Word embedding dimension\n",
        "CHAR_EMBEDDING_DIM = 30  # Character embedding dimension\n",
        "HIDDEN_DIM = 256  # LSTM hidden dimension\n",
        "LSTM_LAYERS = 1  # Number of LSTM layers\n",
        "CNN_OUT_DIM = 50  # CNN output dimension\n",
        "CNN_KERNEL_SIZE = 3  # CNN kernel size\n",
        "DROPOUT = 0.1  # Dropout rate\n",
        "LINEAR_OUT_DIM = 128  # Linear layer output dimension\n",
        "BATCH_SIZE = 32  # Batch size\n",
        "LEARNING_RATE = 0.025  # Updated learning rate\n",
        "MOMENTUM = 0.99  # Momentum for SGD\n",
        "WEIGHT_DECAY = 1e-5  # Weight decay\n",
        "EPOCHS = 20  # Number of epochs\n",
        "ALPHA_ELU = 0.01\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    glove_embeddings = {}\n",
        "    with gzip.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "\n",
        "# Load dataset\n",
        "def load_data(filename, is_test=False):\n",
        "    sentences, labels = [], []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence, label = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            sentence.append(parts[1])\n",
        "            if not is_test:\n",
        "                label.append(parts[2] if len(parts) > 2 else \"O\")\n",
        "            else:\n",
        "                label.append(\"O\")\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load train, dev, and test data\n",
        "train_sentences, train_labels = load_data(train_file)\n",
        "dev_sentences, dev_labels = load_data(dev_file)\n",
        "test_sentences, _ = load_data(test_file, is_test=True)\n",
        "\n",
        "# Build vocabulary and character vocabulary\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "char_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "tag_to_ix = {}\n",
        "\n",
        "# Create word and character vocabularies\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        for char in word:\n",
        "            if char not in char_to_ix:\n",
        "                char_to_ix[char] = len(char_to_ix)\n",
        "\n",
        "# Create embedding matrix for words\n",
        "embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_to_ix), EMBEDDING_DIM))\n",
        "\n",
        "for word, idx in word_to_ix.items():\n",
        "    lowercase_word = word.lower()\n",
        "    if lowercase_word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[lowercase_word]\n",
        "    elif word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "\n",
        "# Map entity labels to indexes\n",
        "for labels in train_labels:\n",
        "    for tag in labels:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
        "\n",
        "# Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_ix, char_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.char_to_ix = char_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        word_indices = [self.word_to_ix.get(w, 1) for w in sentence]\n",
        "        char_indices = [[self.char_to_ix.get(c, 1) for c in word] for word in sentence]\n",
        "        label_indices = [self.tag_to_ix.get(l, 0) for l in label]\n",
        "        return torch.tensor(word_indices), char_indices, torch.tensor(label_indices)\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    words, chars, labels = zip(*batch)\n",
        "\n",
        "    # Pad word sequences\n",
        "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad label sequences\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "\n",
        "    # Pad character sequences\n",
        "    max_word_len = max([max([len(word) for word in sentence]) for sentence in chars])  # Longest word in the batch\n",
        "    max_sentence_len = max([len(sentence) for sentence in chars])  # Longest sentence in the batch\n",
        "\n",
        "    # Initialize a tensor for padded character sequences\n",
        "    chars_padded = torch.zeros((len(chars), max_sentence_len, max_word_len), dtype=torch.long)\n",
        "\n",
        "    # Fill the tensor with character indices\n",
        "    for i, sentence in enumerate(chars):\n",
        "        for j, word in enumerate(sentence):\n",
        "            chars_padded[i, j, :len(word)] = torch.tensor(word, dtype=torch.long)\n",
        "\n",
        "    return words_padded, chars_padded, labels_padded\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = NERDataset(train_sentences, train_labels, word_to_ix, char_to_ix, tag_to_ix)\n",
        "dev_dataset = NERDataset(dev_sentences, dev_labels, word_to_ix, char_to_ix, tag_to_ix)\n",
        "test_dataset = NERDataset(test_sentences, [[\"O\"] * len(s) for s in test_sentences], word_to_ix, char_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define LSTM-CNN Model\n",
        "class LSTM_CNN_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, char_vocab_size, tagset_size, embedding_matrix, char_embedding_dim=30):\n",
        "        super(LSTM_CNN_NER, self).__init__()\n",
        "        # Word embeddings\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
        "        # Character embeddings\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=0)\n",
        "        # CNN for character-level features\n",
        "        self.cnn = nn.Conv1d(char_embedding_dim, CNN_OUT_DIM, CNN_KERNEL_SIZE, padding=(CNN_KERNEL_SIZE // 2))\n",
        "\n",
        "        # BLSTM for word-level features\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM + CNN_OUT_DIM, HIDDEN_DIM, num_layers=LSTM_LAYERS,\n",
        "                            bidirectional=True, dropout=DROPOUT, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, LINEAR_OUT_DIM)\n",
        "        self.elu = nn.ELU(alpha=ALPHA_ELU)\n",
        "\n",
        "        # Output layer\n",
        "        self.classifier = nn.Linear(LINEAR_OUT_DIM, tagset_size)\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        # Word embeddings\n",
        "        word_embeds = self.word_embedding(words)\n",
        "        # Character embeddings\n",
        "        batch_size, seq_len, word_len = chars.size()\n",
        "        chars = chars.view(batch_size * seq_len, word_len)\n",
        "        char_embeds = self.char_embedding(chars)\n",
        "        char_embeds = char_embeds.permute(0, 2, 1)\n",
        "        # CNN for character-level features\n",
        "        char_features = torch.relu(self.cnn(char_embeds))  # Apply ReLU\n",
        "        char_features = nn.functional.normalize(char_features, p=2, dim=2)  # Normalize features\n",
        "        char_features, _ = torch.max(char_features, dim=2)  # Max pooling\n",
        "\n",
        "        char_features = char_features.view(batch_size, seq_len, -1)\n",
        "        # Concatenate word and character features\n",
        "        combined_embeds = torch.cat((word_embeds, char_features), dim=2)\n",
        "        # BLSTM\n",
        "        lstm_out, _ = self.lstm(combined_embeds)\n",
        "        # Fully connected layer\n",
        "        fc_out = self.fc(lstm_out)\n",
        "        fc_out = self.elu(fc_out)\n",
        "        # Output layer\n",
        "        logits = self.classifier(fc_out)\n",
        "        return logits\n",
        "\n",
        "# Initialize model, loss, optimizer, and scheduler\n",
        "model = LSTM_CNN_NER(len(word_to_ix), len(char_to_ix), len(tag_to_ix), embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.4)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for words, chars, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(words, chars)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1).long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"blstm3.pt\"))\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, sentences, output_file):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    batch_offset = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, chars, _ in data_loader:\n",
        "            outputs = model(words, chars)\n",
        "            outputs = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for batch_idx, preds in enumerate(outputs):\n",
        "                sentence = sentences[batch_offset]\n",
        "                for word_idx, pred in enumerate(preds[:len(sentence)]):\n",
        "                    predictions.append(f\"{word_idx + 1} {sentence[word_idx]} {ix_to_tag[pred.item()]}\\n\")\n",
        "                predictions.append(\"\\n\")\n",
        "                batch_offset += 1\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(predictions)\n",
        "\n",
        "# Save outputs as dev3.out and test3.out\n",
        "evaluate(model, dev_loader, dev_sentences, os.path.join(output_dir, \"dev3.out\"))\n",
        "evaluate(model, test_loader, test_sentences, os.path.join(output_dir, \"test3.out\"))\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIwYGy5YyTuF",
        "outputId": "8d19090a-9ede-4ab4-aa30-4652da5e97d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 190.5833, LR: 0.025\n",
            "Epoch 2, Loss: 47.7642, LR: 0.025\n",
            "Epoch 3, Loss: 31.7900, LR: 0.025\n",
            "Epoch 4, Loss: 25.1390, LR: 0.025\n",
            "Epoch 5, Loss: 19.3169, LR: 0.010000000000000002\n",
            "Epoch 6, Loss: 13.2859, LR: 0.010000000000000002\n",
            "Epoch 7, Loss: 11.2691, LR: 0.010000000000000002\n",
            "Epoch 8, Loss: 9.5929, LR: 0.010000000000000002\n",
            "Epoch 9, Loss: 8.4127, LR: 0.010000000000000002\n",
            "Epoch 10, Loss: 7.2778, LR: 0.004000000000000001\n",
            "Epoch 11, Loss: 5.5381, LR: 0.004000000000000001\n",
            "Epoch 12, Loss: 4.7083, LR: 0.004000000000000001\n",
            "Epoch 13, Loss: 4.2993, LR: 0.004000000000000001\n",
            "Epoch 14, Loss: 3.9853, LR: 0.004000000000000001\n",
            "Epoch 15, Loss: 3.5638, LR: 0.0016000000000000005\n",
            "Epoch 16, Loss: 3.1695, LR: 0.0016000000000000005\n",
            "Epoch 17, Loss: 2.9620, LR: 0.0016000000000000005\n",
            "Epoch 18, Loss: 2.8661, LR: 0.0016000000000000005\n",
            "Epoch 19, Loss: 2.7542, LR: 0.0016000000000000005\n",
            "Epoch 20, Loss: 2.6139, LR: 0.0006400000000000003\n",
            "Model saved!\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/hw4csci544/eval.py -p /content/drive/MyDrive/hw4csci544/dev3.out -g /content/drive/MyDrive/hw4csci544/data/dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNUyh228_er8",
        "outputId": "b3489795-94f1-45dc-91b7-d14c3d0ba546"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 6110 phrases; correct: 5253.\n",
            "accuracy:  97.90%; precision:  85.97%; recall:  88.40%; FB1:  87.17\n",
            "              LOC: precision:  94.04%; recall:  91.83%; FB1:  92.92  1794\n",
            "             MISC: precision:  76.00%; recall:  82.10%; FB1:  78.94  996\n",
            "              ORG: precision:  76.80%; recall:  83.45%; FB1:  79.99  1457\n",
            "              PER: precision:  90.71%; recall:  91.75%; FB1:  91.23  1863\n"
          ]
        }
      ]
    }
  ]
}